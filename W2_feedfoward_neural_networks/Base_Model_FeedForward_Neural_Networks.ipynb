{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSC1185 Neural Language Modelling: Week 2 Lab -- Feed Forward Neural Networks\n",
        "\n",
        "**Feedforward Neural Networks** (FFNs) are a type of artificial neural network useful for more complex NLP tasks than logistic regression (LR), including both classification and language modelling tasks. FNNs consist of layers of interconnected neurons organised into an input layer, one or more hidden layers, and an output layer. Each connection between nodes is associated with a weight, and each neuron computes the weighted sum of its inputs and applies an activation function to the result which gives the unit's output. See the lecture slides and Jurafsky & Martin, Chapter 6 for details.\n",
        "\n",
        "In this lab, you will code, more or less from scratch, a Feedforward Neural Network and apply it to a topic classification task.\n",
        "\n",
        "A lot of it should look familiar from the Week 1 lab, as each node and its inputs in an FFN can be thought of as a binary logistic regressor (except that the activation function isn't always the sigmoid).\n",
        "\n",
        "As in Week 1, we provide a coding framework with a lot of the code in place, for you to add to. At two points during the lab, we will share partial solutions.\n",
        "\n",
        "The remainder of the lab (for which we don't provide solutions) is for you to complete on your own, and must be submitted for assessment for Week 2 of this module (see handbook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ranSpOffE3VY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Troughout this notebook, additional information can be found in the Week 2 lecture slides and Chapter 6 of the book **[Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/6.pdf)** by Dan Jurafsky and James H. Martin."
      ],
      "metadata": {
        "id": "6XThcvHgkUIN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8M-G8P2Ej5A"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline allows for displaying plots directly in the Jupyter notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# NumPy is a library for numerical computations, with support for arrays and matrices\n",
        "import numpy as np\n",
        "\n",
        "# tqdm is used for creating progress bars to track the progress of for loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Matplotlib is a plotting library, and we use it for tracking the loss values across iterations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# make_classification is a function in the scikit-learn library to generate a random classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# StandardScaler is a function in scikit-learn for normalising (scaling) the features of a dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# train_test_split is a function in scikit-learn for splitting a dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# accuracy_score is a function in scikit-learn that implements the accuracy metric to evaluate classification models\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#TfidfVectorizer class from scikit-learn for text feature extraction using TF-IDF.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# IPython.display allows clearing the output in a Jupyter notebook cell\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Seed for reproducibility, fixing the random seed ensures consistent results when rerunning the code\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1 Feedforward Neural Networks Implementation\n"
      ],
      "metadata": {
        "id": "bl9L4GOoUMRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Activation functions -- definition and derivatives"
      ],
      "metadata": {
        "id": "avU1okK2cYBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our implementation, we'll use Sigmoid and Softmax as activation functions:\n",
        "- **Sigmoid Function:**\n",
        "\n",
        "  $ \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
        "\n",
        "  *Derivative (with respect to *z*):*\n",
        "  $\\frac{d}{dz} \\sigma(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
        "\n",
        "- **Softmax Function:**\n",
        "\n",
        "  $\\text{softmax}(z_k) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$ , Where k is the number of classes\n",
        "\n",
        "  *Derivative (with respect to *z*$_k$):*\n",
        "  $\\frac{\\partial}{\\partial z_k} \\text{softmax}(z_k) = \\text{softmax}(z_k) \\cdot (1 - \\text{softmax}(z_k))$\n",
        "\n",
        "**Note:** As discussed in the lecture, we require the derivatives of all\n",
        " function involved in forward propagation. These derivatives indicate how each weight and bias needs to be changed to minimise the loss function, forming the basis for the backward propagation loop for updating the weights and biases."
      ],
      "metadata": {
        "id": "iCbY_RSeda0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Backward propagation\n",
        "\n",
        "The goal of backward propagation is to calculate the change in each weight and bias according to the loss we receive from the forward propagation. We need to change the weights and biases individually, because the loss is a function of all the weights and biases (i.e. many variables), so we use partial derivatives with respect to each weight and bias.\n",
        "\n",
        "To obtain partial derivative, we need to remember that a neural network is a composition of layers and each layer applies an activation function to the weighted sum of its inputs in order to compute its output. To get the derivative of each weight and bias, we apply a technique called the chain rule (see lecture slides and Jurafsky & Martin).\n",
        "\n",
        "The chain rule is a fundamental rule in calculus that describes how to find derivatives of composite functions. Mathematically, it can be stated as follows.\n",
        "\n",
        "For example, suppose we have two functions, $u$ and $v$, and their composition $f(x) = u(v(x))$. If both $u$ and $v$ are differentiable, then the derivative of $f$ with respect to $x$ is given by the product of the derivative of $u$ with respect to $v(x)$ and the derivative of $v$ with respect to $x$:\n",
        "\n",
        "$\\frac{df}{dx} = \\frac{du}{dv} \\cdot \\frac{dv}{dx}$\n",
        "\n",
        "The chain rule can be extended to more than two functions, for example consider $f(x) = u(v(w(x)))$, the derivative of $f(x)$ is:\n",
        "\n",
        "$\\frac{df}{dx} = \\frac{du}{dv} \\cdot \\frac{dv}{dw} \\cdot \\frac{dw}{dx}$"
      ],
      "metadata": {
        "id": "wQ39eyqhcwcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the forward propagation, for a 3-layer FFN with sigmoid in the first two layers and softmax in the last layer we have:\n",
        "\n",
        "$z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$\n",
        "\n",
        "$a^{[1]} = \\text{sigmoid}(z^{[1]})$\n",
        "\n",
        "$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$\n",
        "\n",
        "$a^{[2]} = \\text{sigmoid}(z^{[2]})$\n",
        "\n",
        "$z^{[3]} = W^{[3]}a^{[2]} + b^{[3]}$\n",
        "\n",
        "$a^{[3]} = \\text{softmax}(z^{[3]})$"
      ],
      "metadata": {
        "id": "xPfeAKUoguj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, in the backward propagation step, we carry out the following backward differentiation computations to obtain our gradients for each weight and bias in each of the three layers (in the lecture, we did this also via computation graphs; for additional explanation see Jurafsky & Martin):"
      ],
      "metadata": {
        "id": "NXmwNNF8h9KP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Output Layer:**\n",
        "\n",
        "* *Derivative (with respect to $(z^{[3]}$)):*\n",
        "\n",
        "    $\\frac{\\partial L}{\\partial z^{[3]}} = \\frac{\\partial \\text{softmax}}{\\partial z^{[3]}} = \\text{softmax}(z^{[3]}) - y_{\\text{true}}$\n",
        "\n",
        "* *Derivative (with respect to $(W^{[3]}$)):*\n",
        "\n",
        "    $\\frac{\\partial L}{\\partial W^{[3]}} = \\frac{\\partial z^{[3]}}{\\partial W^{[3]}} \\cdot \\frac{\\partial L}{\\partial z^{[3]}} = (a^{[2]})^T \\cdot \\frac{\\partial L}{\\partial z^{[3]}}$\n",
        "\n",
        "* *Derivative (with respect to $(b^{[3]}$)):*\n",
        "\n",
        "    $\\frac{\\partial L}{\\partial b^{[3]}} = \\frac{\\partial z^{[3]}}{\\partial b^{[3]}} \\cdot \\frac{\\partial L}{\\partial z^{[3]}} = \\sum \\frac{\\partial L}{\\partial z^{[3]}}$\n",
        "\n",
        "**Hidden Layer 2:**\n",
        "\n",
        "* *Derivative (with respect to $(z^{[2]}$)):*\n",
        "\n",
        "    $\\frac{\\partial L}{\\partial z^{[2]}} = \\frac{\\partial L}{\\partial z^{[3]}} \\cdot (W^{[3]})^T \\cdot \\frac{\\partial \\text{sigmoid}}{\\partial z^{[2]}} = \\frac{\\partial L}{\\partial z^{[3]}} \\cdot (W^{[3]})^T \\cdot \\text{sigmoid}'(z^{[2]})$\n",
        "\n",
        "where $\\frac{\\partial L}{\\partial z^{[3]}} \\cdot (W^{[3]})^T$ is the error from Output Layer and $\\text{sigmoid}'$ is the derivative of the sigmoid function.\n",
        "\n",
        "* *Derivative (with respect to $(W^{[2]}$)):*\n",
        "\n",
        "    $\\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial W^{[2]}} \\cdot \\frac{\\partial L}{\\partial z^{[2]}} = (a^{[1]})^T \\cdot \\frac{\\partial L}{\\partial z^{[2]}}$\n",
        "\n",
        "* *Derivative (with respect to $(b^{[2]}$)):*\n",
        "\n",
        "    $\\frac{\\partial L}{\\partial b^{[2]}} = \\frac{\\partial z^{[2]}}{\\partial b^{[2]}} \\cdot \\frac{\\partial L}{\\partial z^{[2]}} = \\sum \\frac{\\partial L}{\\partial z^{[2]}}$\n",
        "\n",
        "**Hidden Layer 1:**\n",
        "\n",
        "* *Derivative (with respect to $(z^{[1]}$)):*\n",
        "\n",
        "    $\\frac{\\partial L}{\\partial z^{[1]}} = \\frac{\\partial L}{\\partial z^{[2]}} \\cdot (W^{[2]})^T \\cdot \\frac{\\partial \\text{sigmoid}}{\\partial z^{[1]}} = \\frac{\\partial L}{\\partial z^{[2]}} \\cdot (W^{[2]})^T \\cdot \\text{sigmoid}'(z^{[1]})$\n",
        "\n",
        "where $\\frac{\\partial L}{\\partial z^{[2]}} \\cdot (W^{[2]})^T$ is the error from Hidden Layer 2.\n",
        "\n",
        "* *Derivative (with respect to $(W^{[1]})$):*\n",
        "\n",
        "    $\\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial z^{[1]}}{\\partial W^{[1]}} \\cdot \\frac{\\partial L}{\\partial z^{[1]}} = X^T \\cdot \\frac{\\partial L}{\\partial z^{[1]}}$\n",
        "\n",
        "* *Derivative (with respect to $(b^{[1]})$):*\n",
        "\n",
        "    $\\frac{\\partial L}{\\partial b^{[1]}} = \\frac{\\partial z^{[1]}}{\\partial b^{[1]}} \\cdot \\frac{\\partial L}{\\partial z^{[1]}} = \\sum \\frac{\\partial L}{\\partial z^{[1]}}$\n"
      ],
      "metadata": {
        "id": "Sk6zmYKydYJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Network definition"
      ],
      "metadata": {
        "id": "-lEs6n7DcZOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To recap from the lecture, creating a feedforward neural network and training it involves the following steps:\n",
        "\n",
        "1. **Initialise Layers:**\n",
        "   - For each layer, initialise a weight matrix and a bias vector:\n",
        "      - The weight matrix (**W**) size is (input features size, output features size).\n",
        "      - The bias vector (**b**) size is (1, output features size).\n",
        "\n",
        "2. **Define Activation Functions for Hidden Layers:**\n",
        "   - Choose activation functions for hidden layers. Common choices include:\n",
        "      - [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)\n",
        "      - [Softmax](https://en.wikipedia.org/wiki/Softmax_function)\n",
        "      - [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
        "\n",
        "3. **Define Forward Propagation Function:**\n",
        "   - Implement a function that computes the output of the neural network given an input by applying activation functions to the weighted sums of inputs at each layer.\n",
        "\n",
        "4. **Define Loss Function:**\n",
        "   - Choose a loss function that measures the distance between predicted and gold outputs. Common choices include mean squared error for regression tasks and cross-entropy loss for classification tasks.\n",
        "\n",
        "5. **Define Backward Propagation Function:**\n",
        "   - Implement a function that computes the gradients of the loss with respect to the weights and biases by applying the chain rule of calculus.\n",
        "\n",
        "6. **Define Training Loop:**\n",
        "   - Iterate through the training data multiple times, performing the following in each iteration:\n",
        "      - Forward pass to compute predictions.\n",
        "      - Compute loss using the chosen loss function.\n",
        "      - Backward pass to compute gradients.\n",
        "      - Update weights and biases using an optimisation algorithm (e.g., gradient descent).\n",
        "      - Repeat until the loss converges or a predefined number of iterations are reached.\n",
        "\n",
        "It's good practice to implement neural network architectures using the class data structure in Python for reusability. This allows training multiple instances with different hyperparameters and facilitates debugging in case of issues. Following this practice, below we implement our feedforward neural network as a class, incorporating the steps above."
      ],
      "metadata": {
        "id": "z01H-lJfb-uA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** We will label our neural network weights and biases using the following notational convention: **W1**, **b1**, which denote the weights and bias of the first (hidden) layer, respectively. Also, we have **z1** and **a1** denoting the weighted sum of the input plus the bias (result of multiplying the layer input with the layer weights and adding the bias term), and the activation (result of passing **z1** through the activation function), of the first layer, respectively. When calculating gradients, we will use the prefix 'd' to indicate a derivative.\n",
        "\n",
        "The cell below contains your first coding task for this lab where in the `forward` function you need to add code to compute Z_i and a_i values for each layer i."
      ],
      "metadata": {
        "id": "6K39Bha0Bo-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedforwardNeuralNetwork:\n",
        "    # Step 1: Initialise weights and biases\n",
        "    def __init__(self, input_size: int, hidden_size1: int, hidden_size2: int, output_size: int) -> None:\n",
        "        '''\n",
        "        Initialises the neural network with random weights and zero biases.\n",
        "\n",
        "        Args:\n",
        "          input_size (int): Number of features in the input data.\n",
        "          hidden_size1 (int): Number of neurons in the first hidden layer.\n",
        "          hidden_size2 (int): Number of neurons in the second hidden layer.\n",
        "          output_size (int): Number of classes in the output layer.\n",
        "        '''\n",
        "\n",
        "        # Layer 1\n",
        "        # Layer 1 weights matrix: (input_size, hidden_size1)\n",
        "        self.W1 = np.random.randn(input_size, hidden_size1)\n",
        "        print(\"Weights layer 1 shape:\", self.W1.shape) # -> (1500, 64)\n",
        "        self.b1 = np.zeros((1, hidden_size1))\n",
        "\n",
        "        # Layer 2\n",
        "        # Layer 2 weights matrix: (hidden_size1, hidden_size2)\n",
        "        self.W2 = np.random.randn(hidden_size1, hidden_size2)\n",
        "        print(\"Weights layer 2 shape:\", self.W2.shape) # -> (64, 32)\n",
        "        self.b2 = np.zeros((1, hidden_size2))\n",
        "\n",
        "        # Layer 3 (Output layer)\n",
        "        # Layer 3 weights matrix: (hidden_size2, output_size)\n",
        "        self.W3 = np.random.randn(hidden_size2, output_size)\n",
        "        print(\"Weights layer 3 shape:\", self.W3.shape) # -> (32, 3)\n",
        "        self.b3 = np.zeros((1, output_size))\n",
        "\n",
        "\n",
        "    # Step 2: Activation Functions\n",
        "    # 2.1: activation functions\n",
        "    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "          Sigmoid function that gives us the predictions of the given samples.\n",
        "\n",
        "          Args:\n",
        "              Z (array): each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
        "\n",
        "          Returns:\n",
        "              np.ndarray: array containing a prediction for each sample.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def softmax(self, z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "          Softmax function that gives us the predictions of the given samples.\n",
        "\n",
        "          Args:\n",
        "              Z (array): each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
        "\n",
        "          Returns:\n",
        "              np.ndarray: array containing a prediction for each sample.\n",
        "        \"\"\"\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    # 2.2: derivatives of activation functions\n",
        "    def d_sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
        "      return x * (1 - x)\n",
        "\n",
        "    # Step 3: Perform Forward propagation\n",
        "    def forward(self, X: np.ndarray, epoch: int=0) -> np.ndarray:\n",
        "        '''\n",
        "        Performs forward propagation for the neural network.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): Input data with dimensions (num_samples, num_features).\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Output predictions after passing through the network's layers.\n",
        "        '''\n",
        "\n",
        "        ## INSERT YOUR CODE HERE ##\n",
        "\n",
        "        # Layer 1\n",
        "\n",
        "        # Compute Z for Layer 1 using input data and Layer 1 weights\n",
        "        # X: (num_train_samples, num_features) x weights layer 1: (input_size, hidden_size1) -> Z1: (num_train_samples, hidden_size1)\n",
        "        Z1 =\n",
        "\n",
        "        # Compute the output of Layer 1 using the activation function of Layer 1 and Layer 1 Z\n",
        "        # Z1: (num_train_samples, hidden_size1) -> a1: (num_train_samples, hidden_size1)\n",
        "        self.a1 =\n",
        "\n",
        "        if epoch == 0:\n",
        "          print(\"Z1 shape (num_train_samples, hidden_size1):\", Z1.shape)  # -> (4500, 64)\n",
        "          print(\"a1 shape (num_train_samples, hidden_size1):\", self.a1.shape)  # -> (4500, 64)\n",
        "\n",
        "\n",
        "        # Layer 2\n",
        "\n",
        "        # Compute Z for Layer 2 using output of Layer 1 and Layer 2 weights\n",
        "        # a1: (num_train_samples, hidden_size1) x weights layer 2: (hidden_size1, hidden_size2) -> Z2: (num_train_samples, hidden_size2)\n",
        "        Z2 =\n",
        "\n",
        "        # Compute the output of Layer 2 using the activation function of Layer 2 and Layer 2 Z\n",
        "        # Z2: (num_train_samples, hidden_size2) -> a2: (num_train_samples, hidden_size2)\n",
        "        self.a2 =\n",
        "\n",
        "        if epoch == 0:\n",
        "          print(\"Z2 shape (num_train_samples, hidden_size2):\", Z2.shape)  # -> (4500, 32)\n",
        "          print(\"a2 shape (num_train_samples, hidden_size2):\", self.a2.shape)  # -> (4500, 32)\n",
        "\n",
        "\n",
        "        # Layer 3 (Output Layer)\n",
        "\n",
        "        # Compute Z for Layer 3 using output of Layer 2 and Layer 3 weights\n",
        "        # a2: (num_train_samples, hidden_size2) x weights layer 3: (hidden_size2, output_size) -> Z3: (num_train_samples, output_size)\n",
        "        Z3 =\n",
        "\n",
        "        # Compute the output of Layer 3 (output of the network) using the activation function of Layer 3 and Layer 3 Z\n",
        "        # Z3: (num_train_samples, output_size) -> a3: (num_train_samples, output_size)\n",
        "        self.a3 =\n",
        "\n",
        "        if epoch == 0:\n",
        "          print(\"Z3 shape (num_train_samples, output_size):\", Z3.shape)  # -> (4500, 3)\n",
        "          print(\"a3 shape (num_train_samples, output_size):\", self.a3.shape)  # -> (4500, 3)\n",
        "\n",
        "        ## END OF YOUR CODE ##\n",
        "\n",
        "        return self.a3\n",
        "\n",
        "\n",
        "    # Step 4: Cross-Entropy Loss Calculation\n",
        "    def cross_entropy_loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "        '''\n",
        "        Computes the cross-entropy loss between the true labels and predicted probabilities.\n",
        "\n",
        "        Args:\n",
        "          y_true (numpy.ndarray): True class labels with dimensions (num_samples,).\n",
        "          y_pred (numpy.ndarray): Predicted probabilities with dimensions (num_samples, num_classes).\n",
        "\n",
        "        Returns:\n",
        "          float: Cross-entropy loss.\n",
        "        '''\n",
        "        num_samples = len(y_true)\n",
        "        loss = -np.sum(np.log(y_pred[np.arange(num_samples), y_true])) / num_samples\n",
        "        return loss\n",
        "\n",
        "\n",
        "    # Step 6: Perform Backward propagation\n",
        "    def backward(self, X: np.ndarray, y_true: np.ndarray, learning_rate: float=0.01, epoch: int=1) -> None:\n",
        "        '''\n",
        "        Computes the cross-entropy loss between the true labels and predicted probabilities.\n",
        "\n",
        "        Args:\n",
        "          X (numpy.ndarray): Samples in the dataset.\n",
        "          y_true (numpy.ndarray): True class labels with dimensions (num_samples,).\n",
        "          learning_rate (float): Learning rate used to update weights. Deafulto to 0.01.\n",
        "          epoch (int): current epoch during training, used to print matrix weights only for first epoch. Default to 1.\n",
        "        '''\n",
        "\n",
        "        num_samples = len(X)\n",
        "\n",
        "        # 5.1 Compute Gradients for Layer 3 (Output Layer) (d_a3, d_W3, d_b3):\n",
        "        '''\n",
        "        d_a3: Calculate the error at the output layer by subtracting 1\n",
        "         from the predicted probabilities corresponding to the true class.\n",
        "\n",
        "        d_W3: Compute the gradient of the loss with respect to the weights connecting\n",
        "         the third layer (output layer) to the second layer.\n",
        "\n",
        "        d_b3: Compute the gradient of the loss with respect to the biases of the output layer.\n",
        "        '''\n",
        "        d_a3 = self.a3.copy()\n",
        "        d_a3[np.arange(num_samples), y_true] -= 1\n",
        "        d_L3 = d_a3/ num_samples\n",
        "\n",
        "        d_W3 = np.dot(self.a2.T, d_L3)\n",
        "        d_b3 = np.sum(d_L3, axis=0, keepdims=True)\n",
        "        if epoch == 0:\n",
        "          print(\"d_a3 shape (num_train_samples, output_size):\", d_a3.shape)  # -> (4500, 3)\n",
        "          print(\"d_L3 shape (num_train_samples, output_size):\", d_L3.shape)  # -> (4500, 3)\n",
        "          print(\"d_W3 shape (hidden_size2, output_size):\", d_W3.shape)  # -> (32, 3)\n",
        "          print(\"d_b3 shape (1, output_size):\", d_b3.shape)  # -> (1, 3)\n",
        "\n",
        "        # 5.2 Compute Gradients for Layer 2 (d_a2, d_W2, d_b2):\n",
        "        '''\n",
        "        d_a2: Calculate the error at the hidden layer .\n",
        "\n",
        "        d_W2: Compute the gradient of the layer 3 output with respect to the weights connecting\n",
        "         the second layer (hidden layer) to the first layer (input layer).\n",
        "\n",
        "        d_b2: Compute the gradient of the layer 3 output with respect to the biases of the hidden layer.\n",
        "        '''\n",
        "        d_a2 = np.dot(d_L3, self.W3.T)\n",
        "        d_L2 = d_a2 * (self.d_sigmoid(self.a2))\n",
        "\n",
        "        d_W2 = np.dot(self.a1.T, d_L2)\n",
        "        d_b2 = np.sum(d_L2, axis=0, keepdims=True)\n",
        "        if epoch == 0:\n",
        "          print(\"d_a2 shape (num_train_samples, hidden_size2):\", d_a2.shape)  # -> (4500, 32)\n",
        "          print(\"d_L2 shape (num_train_samples, hidden_size2):\", d_L2.shape)  # -> (4500, 32)\n",
        "          print(\"d_W2 shape (hidden_size1, hidden_size2):\", d_W2.shape)  # -> (64, 32)\n",
        "          print(\"d_b2 shape (1, hidden_size2):\", d_b2.shape)  # -> (1, 32)\n",
        "\n",
        "        # 5.3 Compute Gradients for Layer 1 (d_a1, d_W1, d_b1):\n",
        "        d_a1 = np.dot(d_L2, self.W2.T)\n",
        "        d_L1 = d_a1 * (self.d_sigmoid(self.a1))\n",
        "\n",
        "        d_W1 = np.dot(X.T, d_L1)\n",
        "        d_b1 = np.sum(d_L1, axis=0, keepdims=True)\n",
        "        if epoch == 0:\n",
        "          print(\"d_a1 shape (num_train_samples, hidden_size1):\", d_a1.shape)  # -> (4500, 64)\n",
        "          print(\"d_L1 shape (num_train_samples, hidden_size1):\", d_L1.shape)  # -> (4500, 64)\n",
        "          print(\"d_W1 shape (input_size, hidden_size2):\", d_W1.shape)  # -> (1500, 64)\n",
        "          print(\"d_b1 shape (1, input_size):\", d_b1.shape)  # -> (1, 64)\n",
        "\n",
        "        # 5.4 Update weights and biases\n",
        "        self.W1 -= learning_rate * d_W1\n",
        "        self.b1 -= learning_rate * d_b1\n",
        "        self.W2 -= learning_rate * d_W2\n",
        "        self.b2 -= learning_rate * d_b2\n",
        "        self.W3 -= learning_rate * d_W3\n",
        "        self.b3 -= learning_rate * d_b3\n",
        "\n",
        "    # Step 6: Training Loop\n",
        "    def train(self, X: np.ndarray, y_true: np.ndarray, epochs: int=100, learning_rate: float=0.01) -> None:\n",
        "        '''\n",
        "        Trains the neural network using the specified training data.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): Input data with dimensions (num_samples, num_features).\n",
        "            y_true (numpy.ndarray): True class labels with dimensions (num_samples,).\n",
        "            epochs (int): Number of training epochs. Default to 100.\n",
        "            learning_rate (float): Learning rate for weight updates. Default to 0.01.\n",
        "        '''\n",
        "        self.losses = []\n",
        "\n",
        "        # tqdm is a function that given a list, the total number of iterations and a description,\n",
        "        # returns a progressbar showing the time execution\n",
        "        # pbar will be updated to include the loss at each epoch\n",
        "        pbar = tqdm(range(epochs), total=epochs, desc=\"Training\")\n",
        "        for epoch in pbar:\n",
        "              # 6.1 Forward pass\n",
        "              y_pred = self.forward(X, epoch)\n",
        "\n",
        "              # 6.2 Compute loss\n",
        "              loss = self.cross_entropy_loss(y_true, y_pred)\n",
        "              self.losses.append(loss)\n",
        "              # updated the progressbar to include the loss of this epoch\n",
        "              pbar.set_description(f'Training - Epoch {epoch + 1}/{epochs}, Loss: {loss:0.03f}') #0.03f to display the first decimal points\n",
        "\n",
        "              # 6.3 Backward pass\n",
        "              self.backward(X, y_true, learning_rate, epoch)\n",
        "\n",
        "\n",
        "    def show_learning_curve(self) -> None:\n",
        "        '''\n",
        "        Displays the learning curve (loss curve) during training.\n",
        "        '''\n",
        "        plt.plot(self.losses)\n",
        "        plt.xlabel('Epochs') # x axis label\n",
        "        plt.ylabel('Cross-Entropy Loss') # y axis label\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def test(self, X: np.ndarray) -> np.ndarray:\n",
        "        '''\n",
        "        Calculates the predictions for the given samples.\n",
        "\n",
        "        Args:\n",
        "            X (numpy.ndarray): Input data with dimensions (num_samples, num_features).\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Output predictions after passing through the network's layers.\n",
        "        '''\n",
        "        y_pred = self.forward(X)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "SEo-ScWITpv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While constructing your neural network, especially for larger ones that can be expensive to train, best practice is to run it using a dummy dataset. This allows you to ensure that everything is working as intended before committing to training on a real dataset."
      ],
      "metadata": {
        "id": "Y5_nYqXhewiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# Assuming X is the input data (features) and y is the target labels\n",
        "# For simplicity, let's assume X and y have the following shapes\n",
        "X = np.random.randn(4500, 1500)  # Dummy input data\n",
        "y = np.random.randint(0, 3, size=(4500,))  # Dummy target labels (3 classes)\n",
        "\n",
        "print('Shape of dataset:', X.shape)"
      ],
      "metadata": {
        "id": "pVnv3iXEojlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first 10 samples in the dataset\n",
        "X[:10]"
      ],
      "metadata": {
        "id": "GMi2n8l7pBXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the neural network\n",
        "input_size = X.shape[1]\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 32\n",
        "output_size = np.max(y) + 1  # Assuming class labels are 0, 1, 2, ...\n",
        "\n",
        "model = FeedforwardNeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)"
      ],
      "metadata": {
        "id": "1h8FjRBIom70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute a forward step to check your code\n",
        "predictions = model.forward(X)"
      ],
      "metadata": {
        "id": "1qzETcIiovmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After completing the above code, you can execute the entire training:"
      ],
      "metadata": {
        "id": "hFVFEx3loq5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.train(X, y, epochs=250, learning_rate=0.01)\n",
        "model.show_learning_curve()"
      ],
      "metadata": {
        "id": "JhRSEz9UcvI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems our Feedforward neural networks is working as intended, now let's move forward and train a classification model using a real world dataset."
      ],
      "metadata": {
        "id": "tF7iWWMjlnPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Train a text classifcation model with FFNs"
      ],
      "metadata": {
        "id": "Nux409nFfnuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Obtain Ddataset\n",
        "\n",
        "Instead of creating a toy dataset as we did in Section 1.1, in this section we'll use a real world dataset.\n",
        "\n",
        "We use the **[AGnews dataset](https://https://huggingface.co/datasets/ag_news)** composed of news articles categorised by domain of the article. The dataset contains 4 categories: *World*; *Sports*, *Business*, and *Science/Technology*.\n",
        "\n",
        "The dataset is comes ready divided into training and test set containing 120K and 7.6K samples, respectively.\n",
        "\n",
        "For this exercise, we are going to exclude the World category.\n",
        "\n",
        "First, we download all the files composing the dataset."
      ],
      "metadata": {
        "id": "IUOhkrYKmeTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1QfAUt0u4wLZVy2Ta1G90jOLNaqzAw2eW' -O agnews_test.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UsOBTnfch-Su4kqmkzXcIizwJt6NWtXZ' -O agnews_train.csv\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "MDE9UhkXmfuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# labels contained in the dataset\n",
        "labels = ['sports', 'business', 'science']\n",
        "\n",
        "# dataset is saved in a CSV file with no header and each column in separated by comma\n",
        "# the file has the following structure:\n",
        "# gold_label , title , body\n",
        "df = pd.read_csv('agnews_train.csv', header=None)\n",
        "\n",
        "# in the dataset gold labels are given as: 1 (World), 2 (Sports), 3 (Business), 4 (Science/Technology)\n",
        "# we discard all the rows with gold label 1 (World) and we keep all the other rows\n",
        "df = df[df[0] != 1]\n",
        "\n",
        "# we create a 'label' column, subtract 2 from each gold label so we obtain a direct mapping with our list of labels:\n",
        "# 0 -> sports ; 1 -> business ; 2 -> science\n",
        "df['label'] = df[0]-2\n",
        "\n",
        "# we concatenate title and body to obtain a unified text\n",
        "df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
        "# remove the original 3 columns to obtain our final processed dataset, containing 2 columns: label and text\n",
        "# the original 3 columns are the unmatched labels, the titles and the bodies\n",
        "processed_df = df.drop(columns=[0,1,2])\n",
        "processed_df.head(10)"
      ],
      "metadata": {
        "id": "kYHoIM0cf-kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, _, y_train, _ = train_test_split(processed_df['text'], processed_df['label'], test_size=0.95, random_state=seed, stratify=processed_df['label'])"
      ],
      "metadata": {
        "id": "Ht7vv43hvixR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Create Features\n",
        "\n",
        "When working with text data, we need to convert the text into numerical features that can be fed into the model. There are many approaches for representing text data as features. In the Week 1 lab, we used features such as the natural logarithm of text length, and the number of sport-related words, etc. Here, we'll leverage TF-IDF as a more informative feature that can help our model to perform better. TF-IDF reflects the importance of a word in a document relative to a collection of documents (corpus).\n",
        "\n",
        "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for Term Frequency-Inverse Document Frequency, is a numerical statistical technique used to generate features from texts. Let's break down TF-IDF into two components: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
        "\n",
        "**Term Frequency (TF):**\n",
        "TF measures how frequently a term (word) appears in a document. It's a local measure that focuses on the importance of a word within a specific document. The formula for TF is:\n",
        "\n",
        "$\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}$\n",
        "\n",
        "In other terms, it's the ratio of the number of times a specific word occurs in a document to the total number of words in that document. TF values are higher for words that appear frequently in the document.\n",
        "\n",
        "**Inverse Document Frequency (IDF):**\n",
        "IDF measures the importance of a term across the entire corpus. It provides a global measure of the rarity of a term. The formula for IDF is:\n",
        "\n",
        "$\\text{IDF}(t, D) = \\log\\left(\\frac{\\text{Total number of documents in the corpus } N}{\\text{Number of documents containing term } t + 1}\\right)$\n",
        "\n",
        "In other terms, it's the logarithmically scaled inverse fraction of the number of documents containing the term *t* to the total number of documents in the corpus. The addition of 1 in the denominator prevents division by zero when a term does not appear in any document. The logarithmic scaling helps to smooth the values.\n",
        "\n",
        "**TF-IDF:**\n",
        "TF-IDF is the product of TF and IDF. The combination of these two components gives a score that reflects the importance of a word in a specific document relative to the entire corpus. The formula for TF-IDF is:\n",
        "\n",
        "$\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)$\n",
        "\n",
        "In practice, words with a higher TF-IDF score are those that appear frequently in a specific document (high TF) but rarely across the entire corpus (high IDF). This approach emphasises words that are unique to a document while downplaying common words appearing frequently across many documents. [Check out this youtube video](https://youtu.be/zLMEnNbdh4Q?feature=shared).\n",
        "\n",
        "To implement TF-IDF, you can use  [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from scikit-learn."
      ],
      "metadata": {
        "id": "6NZ7veLy4ruy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next coding task is for you to create a `TfidfVectorizer` instance and transform the training data inputs with it. Note that `TfidfVectorizer` will build a vocabulary that only considers the top `max_features` terms ordered by term frequency across the corpus. If `max_features` is not specified, all terms are used."
      ],
      "metadata": {
        "id": "90zRVsPHXzr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct a new feature matrix containing the features calculated using TD-IDF\n",
        "# Create TF-IDF features\n",
        "\n",
        "## INSERT YOUR CODE HERE ##\n",
        "\n",
        "def get_features_text(text: pd.Series, max_features: int):\n",
        "    \"\"\"\n",
        "    Calculate text features for the given text using TF-IDF with the given maximum number of features.\n",
        "\n",
        "    Args:\n",
        "        text (pd.Series): a column of texts in a pandas dataframe.\n",
        "        max_features (int): TF-IDF vectoriser instance.\n",
        "\n",
        "    Returns:\n",
        "        Matrix: TF-IDF Features extracted from the text.\n",
        "    \"\"\"\n",
        "    # Create a TF-IDF vectoriser instance specifying the maximum number of features\n",
        "    tfidf_vectoriser =\n",
        "    # Transform the input text using the created TF-IDF vectoriser\n",
        "    tfidf_features_matrix =\n",
        "\n",
        "    return tfidf_features_matrix\n",
        "\n",
        "## END OF YOUR CODE ##\n",
        "\n",
        "max_features = 1500  # Adjust max_features as needed\n",
        "train_TD_IDF_features =  get_features_text(X_train, max_features)"
      ],
      "metadata": {
        "id": "cDeIQFolwfo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer returns a sparse matrix, we need to convert this matrix into a numpy array to pass it trough our Neural Network\n",
        "# the todense() function allows us to convert the dense matrix into a list that we convert to a numpy array using np.array()\n",
        "X_train = np.array(train_TD_IDF_features.todense())\n",
        "\n",
        "print(f\"Shape of train dataset after computing TD-IDF features for each text: {train_TD_IDF_features.shape}\")\n",
        "# -> (4500, 1500)"
      ],
      "metadata": {
        "id": "yeV5Xw74mQvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Initialise a new instance of FeedforwardNeuralNetwork and set the the proper sizes\n",
        "\n",
        "The next coding task is for you to create a new instance of the `FeedforwardNeuralNetwork` class, instantiate the network with correct layer sizes and train it on the vectorised training data."
      ],
      "metadata": {
        "id": "cEz0qnsCspL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters definition\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 32\n",
        "output_size = np.max(y_train) + 1  # Assuming class labels are 0, 1, 2, ...\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 200\n",
        "\n",
        "## INSERT YOUR CODE HERE ##\n",
        "\n",
        "# Instantiate the network with the correct layer sizes\n",
        "model =\n",
        "\n",
        "# Train the model\n",
        "model\n",
        "\n",
        "## END OF YOUR CODE ##\n",
        "\n",
        "model.show_learning_curve()"
      ],
      "metadata": {
        "id": "krb3L1HXs-bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Evaluate the trained model"
      ],
      "metadata": {
        "id": "Ig3hTbCvxWQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test set to set it in the correct format, i.e.\n",
        "# create y_test containing the labels and X_test containing text features created using TF-IDF.\n",
        "# For computation purposes, we restrict the test to 5% of its original size.\n",
        "# This cell repeats some steps from Section 2.1 to obtain the restricted test set\n",
        "\n",
        "df = pd.read_csv('agnews_test.csv', header=None)\n",
        "df = df[df[0] != 1]\n",
        "df['label'] = df[0]-2\n",
        "\n",
        "df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
        "processed_df = df.drop(columns=[0,1,2])\n",
        "processed_df.head(10)\n",
        "\n",
        "_, X_test, _, y_test = train_test_split(processed_df['text'], processed_df['label'], test_size=0.05, random_state=seed, stratify=processed_df['label'])\n",
        "\n",
        "\n",
        "test_TD_IDF_features = get_features_text(X_test, max_features)\n",
        "\n",
        "X_test = np.array(test_TD_IDF_features.todense())\n",
        "\n",
        "print(f\"Shape of test dataset after computing TD-IDF features for each text: {test_TD_IDF_features.shape}\")\n",
        "# -> (285, 1500)\n",
        "\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "xqc73NsUxaT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on the test set using the trained weights\n",
        "test_predictions = model.test(X_test)\n",
        "\n",
        "# select the class with the highest probability\n",
        "multiclass_predictions = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Evaluating the model using Accuracy (as seen in Week 1 Lab)\n",
        "accuracy = accuracy_score(y_test, multiclass_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "rtpwRs0CyiNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Improve performance (Submission exercise)\n",
        "\n",
        "Try to improve the performance of the network in Section 2, e.g. by changing hyperparameters, finding a better weight initialisation, and changing the activation function in (hidden) Layer 1, or possibly using a bigger portion of the AGNews dataset for training.\n",
        "\n",
        "If you change the activation function, remember that you need to also change the derivative of that function in the backpropagation step.\n",
        "\n",
        "When you're done, copy all and only the required code to a new notebook, and submit it for Week 2, by placing it in a new Week 2 subfolder in your previously shared submission folder."
      ],
      "metadata": {
        "id": "bcwUSghLNGvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "- Speech and Language Processing (3rd ed. draft) Dan Jurafsky and James H. Martin, [Chapter 6](https://web.stanford.edu/~jurafsky/slp3/6.pdf)"
      ],
      "metadata": {
        "id": "LnF13fbPIark"
      }
    }
  ]
}