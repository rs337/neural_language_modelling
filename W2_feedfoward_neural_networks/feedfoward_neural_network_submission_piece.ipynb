{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79ccb53",
   "metadata": {},
   "source": [
    "### Results\n",
    "> starting accuracy = 0.3438\n",
    "\n",
    "> final accuracy = 0.8169"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7cd22e",
   "metadata": {},
   "source": [
    "# Improvements \n",
    "- In chapter 6, it is noted that sigmoid and tanh use can lead to vanishing gradients in hidden layers(high input values lead to \"saturated\" outputs where derivative is close to 0). Therefore, ReLU may be a better option\n",
    "- In the output layer we will continue to use softmax as it is a multiclass problem, we would use sigmoud if it was a binary problem.\n",
    "- Standardising features achieved ~+1.0% accuracy\n",
    "> learning_rate\n",
    "\n",
    "    - 0.01 = 40%\n",
    "    - 0.05 = 40%\n",
    "    - 0.1 = 37%\n",
    "    - 0.5 = 33%\n",
    "\n",
    "- updated `get_features_text()`\n",
    "    - Previously I had been using fit_transform on the get_features() implementation for train and test. Although you only want to initialise a new tfidf vectoriser once (for the train) and then for the test, you need to feed in the same vectoriser used for the train data, this is to ensure that both the train and test use the same vocabularly. Otherwise, the same word might map to different feature indices (or not exist at all) between train and test, causing the model to receive misaligned inputs at test time\n",
    "- Increasing the size of the hidden layers had a huge benefit, the original setup had 64,32 givign about 50% but with 128,128 I get 69% accuracy\n",
    "- 256,128 layout for the hidden layers brought the accuracy up to 81% where I stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808b662",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "d21f1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import clear_output\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b0e59",
   "metadata": {},
   "source": [
    "### Class initialisation\n",
    "\n",
    "To recap from the lecture, creating a feedforward neural network and training it involves the following steps:\n",
    "\n",
    "1. **Initialise Layers:**\n",
    "   - For each layer, initialise a weight matrix and a bias vector:\n",
    "      - The weight matrix (**W**) size is (input features size, output features size).\n",
    "      - The bias vector (**b**) size is (1, output features size).\n",
    "\n",
    "2. **Define Activation Functions for Hidden Layers:**\n",
    "   - Choose activation functions for hidden layers. Common choices include:\n",
    "      - [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)\n",
    "      - [Softmax](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "      - [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "3. **Define Forward Propagation Function:**\n",
    "   - Implement a function that computes the output of the neural network given an input by applying activation functions to the weighted sums of inputs at each layer.\n",
    "\n",
    "4. **Define Loss Function:**\n",
    "   - Choose a loss function that measures the distance between predicted and gold outputs. Common choices include mean squared error for regression tasks and cross-entropy loss for classification tasks.\n",
    "\n",
    "5. **Define Backward Propagation Function:**\n",
    "   - Implement a function that computes the gradients of the loss with respect to the weights and biases by applying the chain rule of calculus.\n",
    "\n",
    "6. **Define Training Loop:**\n",
    "   - Iterate through the training data multiple times, performing the following in each iteration:\n",
    "      - Forward pass to compute predictions.\n",
    "      - Compute loss using the chosen loss function.\n",
    "      - Backward pass to compute gradients.\n",
    "      - Update weights and biases using an optimisation algorithm (e.g., gradient descent).\n",
    "      - Repeat until the loss converges or a predefined number of iterations are reached.\n",
    "\n",
    "It's good practice to implement neural network architectures using the class data structure in Python for reusability. This allows training multiple instances with different hyperparameters and facilitates debugging in case of issues. Following this practice, below we implement our feedforward neural network as a class, incorporating the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "80c368ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetwork:\n",
    "    # Step 1: Initialise weights and biases\n",
    "    def __init__(self, input_size: int, hidden_size1: int, hidden_size2: int, output_size: int) -> None:\n",
    "        '''\n",
    "        Initialises the neural network with random weights and zero biases.\n",
    "\n",
    "        Args:\n",
    "          input_size (int): Number of features in the input data.\n",
    "          hidden_size1 (int): Number of neurons in the first hidden layer.\n",
    "          hidden_size2 (int): Number of neurons in the second hidden layer.\n",
    "          output_size (int): Number of classes in the output layer.\n",
    "        '''\n",
    "\n",
    "        # Layer 1\n",
    "        # Layer 1 weights matrix: (input_size, hidden_size1)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size1)\n",
    "        print(\"Weights layer 1 shape:\", self.W1.shape) # -> (1500, 64)\n",
    "        self.b1 = np.zeros((1, hidden_size1))\n",
    "\n",
    "        # Layer 2\n",
    "        # Layer 2 weights matrix: (hidden_size1, hidden_size2)\n",
    "        self.W2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "        print(\"Weights layer 2 shape:\", self.W2.shape) # -> (64, 32)\n",
    "        self.b2 = np.zeros((1, hidden_size2))\n",
    "\n",
    "        # Layer 3 (Output layer)\n",
    "        # Layer 3 weights matrix: (hidden_size2, output_size)\n",
    "        self.W3 = np.random.randn(hidden_size2, output_size)\n",
    "        print(\"Weights layer 3 shape:\", self.W3.shape) # -> (32, 3)\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "\n",
    "\n",
    "    # Step 2: Activation Functions\n",
    "    # 2.1: activation functions\n",
    "    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "          Sigmoid function that gives us the predictions of the given samples.\n",
    "\n",
    "          Args:\n",
    "              Z (array): each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
    "\n",
    "          Returns:\n",
    "              np.ndarray: array containing a prediction for each sample.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def reLU(self, z:np.ndarray) -> np.ndarray:\n",
    "      '''\n",
    "      np.maximum checks each value in Z and compares it against 0, would not work with max\n",
    "      ''' \n",
    "      return np.maximum(0, z)\n",
    "\n",
    "    def softmax(self, z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "          Softmax function that gives us the predictions of the given samples.\n",
    "\n",
    "          Args:\n",
    "              Z (array): each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
    "\n",
    "          Returns:\n",
    "              np.ndarray: array containing a prediction for each sample.\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    # 2.2: derivatives of activation functions\n",
    "    def d_sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "      return x * (1 - x)\n",
    "    \n",
    "    def d_reLU(self, z: np.ndarray) -> np.ndarray:\n",
    "    # z >= 0 checks if each element of z is greater than 0, if true, astype(float), converts it to 1, false = 0\n",
    "      return (z >= 0).astype(float)\n",
    "\n",
    "    # Step 3: Perform Forward propagation\n",
    "    def forward(self, X: np.ndarray, epoch: int=0) -> np.ndarray:\n",
    "        '''\n",
    "        Performs forward propagation for the neural network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with dimensions (num_samples, num_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output predictions after passing through the network's layers.\n",
    "        '''\n",
    "\n",
    "        ## INSERT YOUR CODE HERE ##\n",
    "        # Layer 1\n",
    "        # Compute Z for Layer 1 using input data and Layer 1 weights\n",
    "        # X: (num_train_samples, num_features) x weights layer 1: (input_size, hidden_size1) -> Z1: (num_train_samples, hidden_size1)\n",
    "        Z1 = np.dot(X, self.W1) + self.b1\n",
    "\n",
    "        # Compute the output of Layer 1 using the activation function of Layer 1 and Layer 1 Z\n",
    "        # Z1: (num_train_samples, hidden_size1) -> a1: (num_train_samples, hidden_size1)\n",
    "        self.a1 = self.reLU(Z1)\n",
    "\n",
    "        if epoch == 0:\n",
    "          print(\"Z1 shape (num_train_samples, hidden_size1):\", Z1.shape)  # -> (4500, 64)\n",
    "          print(\"a1 shape (num_train_samples, hidden_size1):\", self.a1.shape)  # -> (4500, 64)\n",
    "\n",
    "\n",
    "        # Layer 2\n",
    "\n",
    "        # Compute Z for Layer 2 using output of Layer 1 and Layer 2 weights\n",
    "        # a1: (num_train_samples, hidden_size1) x weights layer 2: (hidden_size1, hidden_size2) -> Z2: (num_train_samples, hidden_size2)\n",
    "        Z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "\n",
    "        # Compute the output of Layer 2 using the activation function of Layer 2 and Layer 2 Z\n",
    "        # Z2: (num_train_samples, hidden_size2) -> a2: (num_train_samples, hidden_size2)\n",
    "        self.a2 = self.reLU(Z2)\n",
    "\n",
    "        if epoch == 0:\n",
    "          print(\"Z2 shape (num_train_samples, hidden_size2):\", Z2.shape)  # -> (4500, 32)\n",
    "          print(\"a2 shape (num_train_samples, hidden_size2):\", self.a2.shape)  # -> (4500, 32)\n",
    "\n",
    "\n",
    "        # Layer 3 (Output Layer)\n",
    "\n",
    "        # Compute Z for Layer 3 using output of Layer 2 and Layer 3 weights\n",
    "        # a2: (num_train_samples, hidden_size2) x weights layer 3: (hidden_size2, output_size) -> Z3: (num_train_samples, output_size)\n",
    "        Z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "\n",
    "        # Compute the output of Layer 3 (output of the network) using the activation function of Layer 3 and Layer 3 Z\n",
    "        # Z3: (num_train_samples, output_size) -> a3: (num_train_samples, output_size)\n",
    "        self.a3 = self.softmax(Z3)\n",
    "\n",
    "        if epoch == 0:\n",
    "          print(\"Z3 shape (num_train_samples, output_size):\", Z3.shape)  # -> (4500, 3)\n",
    "          print(\"a3 shape (num_train_samples, output_size):\", self.a3.shape)  # -> (4500, 3)\n",
    "\n",
    "        ## END OF YOUR CODE ##\n",
    "\n",
    "        return self.a3\n",
    "\n",
    "\n",
    "    # Step 4: Cross-Entropy Loss Calculation\n",
    "    def cross_entropy_loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        '''\n",
    "        Computes the cross-entropy loss between the true labels and predicted probabilities.\n",
    "\n",
    "        Args:\n",
    "          y_true (numpy.ndarray): True class labels with dimensions (num_samples,).\n",
    "          y_pred (numpy.ndarray): Predicted probabilities with dimensions (num_samples, num_classes).\n",
    "\n",
    "        Returns:\n",
    "          float: Cross-entropy loss.\n",
    "        '''\n",
    "        num_samples = len(y_true)\n",
    "        loss = -np.sum(np.log(y_pred[np.arange(num_samples), y_true])) / num_samples\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # Step 6: Perform Backward propagation\n",
    "    def backward(self, X: np.ndarray, y_true: np.ndarray, learning_rate: float=0.01, epoch: int=1) -> None:\n",
    "        '''\n",
    "        Computes the cross-entropy loss between the true labels and predicted probabilities.\n",
    "\n",
    "        Args:\n",
    "          X (numpy.ndarray): Samples in the dataset.\n",
    "          y_true (numpy.ndarray): True class labels with dimensions (num_samples,).\n",
    "          learning_rate (float): Learning rate used to update weights. Deafulto to 0.01.\n",
    "          epoch (int): current epoch during training, used to print matrix weights only for first epoch. Default to 1.\n",
    "        '''\n",
    "\n",
    "        num_samples = len(X)\n",
    "\n",
    "        # 5.1 Compute Gradients for Layer 3 (Output Layer) (d_a3, d_W3, d_b3):\n",
    "        '''\n",
    "        d_a3: Calculate the error at the output layer by subtracting 1\n",
    "         from the predicted probabilities corresponding to the true class.\n",
    "\n",
    "        d_W3: Compute the gradient of the loss with respect to the weights connecting\n",
    "         the third layer (output layer) to the second layer.\n",
    "\n",
    "        d_b3: Compute the gradient of the loss with respect to the biases of the output layer.\n",
    "        '''\n",
    "        d_a3 = self.a3.copy()\n",
    "        d_a3[np.arange(num_samples), y_true] -= 1\n",
    "        d_L3 = d_a3/ num_samples\n",
    "\n",
    "        d_W3 = np.dot(self.a2.T, d_L3)\n",
    "        d_b3 = np.sum(d_L3, axis=0, keepdims=True)\n",
    "        if epoch == 0:\n",
    "          print(\"d_a3 shape (num_train_samples, output_size):\", d_a3.shape)  # -> (4500, 3)\n",
    "          print(\"d_L3 shape (num_train_samples, output_size):\", d_L3.shape)  # -> (4500, 3)\n",
    "          print(\"d_W3 shape (hidden_size2, output_size):\", d_W3.shape)  # -> (32, 3)\n",
    "          print(\"d_b3 shape (1, output_size):\", d_b3.shape)  # -> (1, 3)\n",
    "\n",
    "        # 5.2 Compute Gradients for Layer 2 (d_a2, d_W2, d_b2):\n",
    "        '''\n",
    "        d_a2: Calculate the error at the hidden layer .\n",
    "\n",
    "        d_W2: Compute the gradient of the layer 3 output with respect to the weights connecting\n",
    "         the second layer (hidden layer) to the first layer (input layer).\n",
    "\n",
    "        d_b2: Compute the gradient of the layer 3 output with respect to the biases of the hidden layer.\n",
    "        '''\n",
    "        d_a2 = np.dot(d_L3, self.W3.T)\n",
    "        d_L2 = d_a2 * (self.d_reLU(self.a2))\n",
    "\n",
    "        d_W2 = np.dot(self.a1.T, d_L2)\n",
    "        d_b2 = np.sum(d_L2, axis=0, keepdims=True)\n",
    "        if epoch == 0:\n",
    "          print(\"d_a2 shape (num_train_samples, hidden_size2):\", d_a2.shape)  # -> (4500, 32)\n",
    "          print(\"d_L2 shape (num_train_samples, hidden_size2):\", d_L2.shape)  # -> (4500, 32)\n",
    "          print(\"d_W2 shape (hidden_size1, hidden_size2):\", d_W2.shape)  # -> (64, 32)\n",
    "          print(\"d_b2 shape (1, hidden_size2):\", d_b2.shape)  # -> (1, 32)\n",
    "\n",
    "        # 5.3 Compute Gradients for Layer 1 (d_a1, d_W1, d_b1):\n",
    "        d_a1 = np.dot(d_L2, self.W2.T)\n",
    "        d_L1 = d_a1 * (self.d_reLU(self.a1))\n",
    "\n",
    "        d_W1 = np.dot(X.T, d_L1)\n",
    "        d_b1 = np.sum(d_L1, axis=0, keepdims=True)\n",
    "        if epoch == 0:\n",
    "          print(\"d_a1 shape (num_train_samples, hidden_size1):\", d_a1.shape)  # -> (4500, 64)\n",
    "          print(\"d_L1 shape (num_train_samples, hidden_size1):\", d_L1.shape)  # -> (4500, 64)\n",
    "          print(\"d_W1 shape (input_size, hidden_size2):\", d_W1.shape)  # -> (1500, 64)\n",
    "          print(\"d_b1 shape (1, input_size):\", d_b1.shape)  # -> (1, 64)\n",
    "\n",
    "        # 5.4 Update weights and biases\n",
    "        self.W1 -= learning_rate * d_W1\n",
    "        self.b1 -= learning_rate * d_b1\n",
    "        self.W2 -= learning_rate * d_W2\n",
    "        self.b2 -= learning_rate * d_b2\n",
    "        self.W3 -= learning_rate * d_W3\n",
    "        self.b3 -= learning_rate * d_b3\n",
    "\n",
    "    # Step 6: Training Loop\n",
    "    def train(self, X: np.ndarray, y_true: np.ndarray, epochs: int=100, learning_rate: float=0.01) -> None:\n",
    "        '''\n",
    "        Trains the neural network using the specified training data.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with dimensions (num_samples, num_features).\n",
    "            y_true (numpy.ndarray): True class labels with dimensions (num_samples,).\n",
    "            epochs (int): Number of training epochs. Default to 100.\n",
    "            learning_rate (float): Learning rate for weight updates. Default to 0.01.\n",
    "        '''\n",
    "        self.losses = []\n",
    "\n",
    "        # tqdm is a function that given a list, the total number of iterations and a description,\n",
    "        # returns a progressbar showing the time execution\n",
    "        # pbar will be updated to include the loss at each epoch\n",
    "        pbar = tqdm(range(epochs), total=epochs, desc=\"Training\")\n",
    "        for epoch in pbar:\n",
    "              # 6.1 Forward pass\n",
    "              y_pred = self.forward(X, epoch)\n",
    "\n",
    "              # 6.2 Compute loss\n",
    "              loss = self.cross_entropy_loss(y_true, y_pred)\n",
    "              self.losses.append(loss)\n",
    "              # updated the progressbar to include the loss of this epoch\n",
    "              pbar.set_description(f'Training - Epoch {epoch + 1}/{epochs}, Loss: {loss:0.03f}') #0.03f to display the first decimal points\n",
    "\n",
    "              # 6.3 Backward pass\n",
    "              self.backward(X, y_true, learning_rate, epoch)\n",
    "\n",
    "\n",
    "    def show_learning_curve(self) -> None:\n",
    "        '''\n",
    "        Displays the learning curve (loss curve) during training.\n",
    "        '''\n",
    "        plt.plot(self.losses)\n",
    "        plt.xlabel('Epochs') # x axis label\n",
    "        plt.ylabel('Cross-Entropy Loss') # y axis label\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def test(self, X: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Calculates the predictions for the given samples.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with dimensions (num_samples, num_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output predictions after passing through the network's layers.\n",
    "        '''\n",
    "        y_pred = self.forward(X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "6a09b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1QfAUt0u4wLZVy2Ta1G90jOLNaqzAw2eW' -O agnews_test.csv\n",
    "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UsOBTnfch-Su4kqmkzXcIizwJt6NWtXZ' -O agnews_train.csv\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "4106689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# labels contained in the dataset\n",
    "labels = ['sports', 'business', 'science']\n",
    "df = pd.read_csv('agnews_train.csv', header=None)\n",
    "\n",
    "# in the dataset gold labels are given as: 1 (World), 2 (Sports), 3 (Business), 4 (Science/Technology)\n",
    "# we discard all the rows with gold label 1 (World) and we keep all the other rows\n",
    "df = df[df[0] != 1]\n",
    "df['label'] = df[0]-2\n",
    "\n",
    "# we concatenate title and body to obtain a unified text\n",
    "df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
    "processed_df = df.drop(columns=[0,1,2])\n",
    "processed_df.head(10)\n",
    "\n",
    "# Split data\n",
    "X_train, _, y_train, _ = train_test_split(processed_df['text'], processed_df['label'], test_size=0.5, random_state=seed, stratify=processed_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "9e09d1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train dataset after computing TD-IDF features for each text: (45000, 1500)\n",
      "Weights layer 1 shape: (1500, 256)\n",
      "Weights layer 2 shape: (256, 128)\n",
      "Weights layer 3 shape: (128, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]/var/folders/wz/ggvm7p353k760lj9f743dp0m0000gn/T/ipykernel_52362/1399927519.py:148: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.sum(np.log(y_pred[np.arange(num_samples), y_true])) / num_samples\n",
      "Training - Epoch 1/200, Loss: inf:   0%|          | 1/200 [00:00<00:37,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 shape (num_train_samples, hidden_size1): (45000, 256)\n",
      "a1 shape (num_train_samples, hidden_size1): (45000, 256)\n",
      "Z2 shape (num_train_samples, hidden_size2): (45000, 128)\n",
      "a2 shape (num_train_samples, hidden_size2): (45000, 128)\n",
      "Z3 shape (num_train_samples, output_size): (45000, 3)\n",
      "a3 shape (num_train_samples, output_size): (45000, 3)\n",
      "d_a3 shape (num_train_samples, output_size): (45000, 3)\n",
      "d_L3 shape (num_train_samples, output_size): (45000, 3)\n",
      "d_W3 shape (hidden_size2, output_size): (128, 3)\n",
      "d_b3 shape (1, output_size): (1, 3)\n",
      "d_a2 shape (num_train_samples, hidden_size2): (45000, 128)\n",
      "d_L2 shape (num_train_samples, hidden_size2): (45000, 128)\n",
      "d_W2 shape (hidden_size1, hidden_size2): (256, 128)\n",
      "d_b2 shape (1, hidden_size2): (1, 128)\n",
      "d_a1 shape (num_train_samples, hidden_size1): (45000, 256)\n",
      "d_L1 shape (num_train_samples, hidden_size1): (45000, 256)\n",
      "d_W1 shape (input_size, hidden_size2): (1500, 256)\n",
      "d_b1 shape (1, input_size): (1, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - Epoch 200/200, Loss: inf: 100%|██████████| 200/200 [00:33<00:00,  5.90it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# construct a new feature matrix containing the features calculated using TD-IDF\n",
    "# Create TF-IDF features\n",
    "\n",
    "## INSERT YOUR CODE HERE ##\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_features_text(text: pd.Series, max_features: int, vectoriser=None):\n",
    "    \"\"\"\n",
    "    Calculate text features for the given text using TF-IDF with the given maximum number of features.\n",
    "\n",
    "    Args:\n",
    "        text (pd.Series): a column of texts in a pandas dataframe.\n",
    "        max_features (int): TF-IDF vectoriser instance.\n",
    "\n",
    "    Returns:\n",
    "        Matrix: TF-IDF Features extracted from the text.\n",
    "    \n",
    "    If vectoriser is None, create and fit a new one (training).\n",
    "    Otherwise, use the provided vectoriser (testing).\n",
    "    \"\"\"\n",
    "    if vectoriser is None:\n",
    "        # Training: create new vectorizer and fit_transform\n",
    "        vectoriser = TfidfVectorizer(max_features=max_features)\n",
    "        tfidf_features_matrix = vectoriser.fit_transform(text)\n",
    "        return tfidf_features_matrix, vectoriser\n",
    "    else:\n",
    "        # Testing: use existing vectorizer, transform only\n",
    "        tfidf_features_matrix = vectoriser.transform(text)\n",
    "        return tfidf_features_matrix\n",
    "\n",
    "## END OF YOUR CODE ##\n",
    "\n",
    "max_features = 1500  # Adjust max_features as needed\n",
    "train_TD_IDF_features, tfidf_vectoriser = get_features_text(X_train, max_features)\n",
    "\n",
    "\n",
    "# TfidfVectorizer returns a sparse matrix, we need to convert this matrix into a numpy array to pass it trough our Neural Network\n",
    "# the todense() function allows us to convert the dense matrix into a list that we convert to a numpy array using np.array()\n",
    "X_train = np.array(train_TD_IDF_features.todense())\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "print(f\"Shape of train dataset after computing TD-IDF features for each text: {train_TD_IDF_features.shape}\")\n",
    "# -> (4500, 1500)\n",
    "\n",
    "# Hyperparameters definition\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = np.max(y_train) + 1  # Assuming class labels are 0, 1, 2, ...\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "\n",
    "## INSERT YOUR CODE HERE ##\n",
    "\n",
    "# Instantiate the network with the correct layer sizes\n",
    "model = FeedforwardNeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Execute a forward step to check your code\n",
    "# predictions = model.forward(X)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "model.train(X_train, y_train, epochs, learning_rate)\n",
    "model.show_learning_curve()\n",
    "\n",
    "## END OF YOUR CODE ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4edb4c",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "654cc5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test dataset after computing TD-IDF features for each text: (5643, 1500)\n",
      "(5643, 1500)\n",
      "Z1 shape (num_train_samples, hidden_size1): (5643, 256)\n",
      "a1 shape (num_train_samples, hidden_size1): (5643, 256)\n",
      "Z2 shape (num_train_samples, hidden_size2): (5643, 128)\n",
      "a2 shape (num_train_samples, hidden_size2): (5643, 128)\n",
      "Z3 shape (num_train_samples, output_size): (5643, 3)\n",
      "a3 shape (num_train_samples, output_size): (5643, 3)\n",
      "Accuracy: 0.8169413432571327\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the test set to set it in the correct format, i.e.\n",
    "# create y_test containing the labels and X_test containing text features created using TF-IDF.\n",
    "# For computation purposes, we restrict the test to 5% of its original size.\n",
    "# This cell repeats some steps from Section 2.1 to obtain the restricted test set\n",
    "\n",
    "df = pd.read_csv('agnews_test.csv', header=None)\n",
    "df = df[df[0] != 1]\n",
    "df['label'] = df[0]-2\n",
    "\n",
    "df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
    "processed_df = df.drop(columns=[0,1,2])\n",
    "processed_df.head(10)\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(processed_df['text'], processed_df['label'], test_size=0.99, random_state=seed, stratify=processed_df['label'])\n",
    "\n",
    "\n",
    "test_TD_IDF_features = get_features_text(X_test, max_features, vectoriser=tfidf_vectoriser)\n",
    "\n",
    "X_test = np.array(test_TD_IDF_features.todense())\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Shape of test dataset after computing TD-IDF features for each text: {test_TD_IDF_features.shape}\")\n",
    "# -> (285, 1500)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "# Making predictions on the test set using the trained weights\n",
    "test_predictions = model.test(X_test)\n",
    "\n",
    "# select the class with the highest probability\n",
    "multiclass_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Evaluating the model using Accuracy (as seen in Week 1 Lab)\n",
    "accuracy = accuracy_score(y_test, multiclass_predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
