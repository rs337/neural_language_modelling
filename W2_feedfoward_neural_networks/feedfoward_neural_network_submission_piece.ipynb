{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4808b662",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import clear_output\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b0e59",
   "metadata": {},
   "source": [
    "### Class initialisation\n",
    "\n",
    "To recap from the lecture, creating a feedforward neural network and training it involves the following steps:\n",
    "\n",
    "1. **Initialise Layers:**\n",
    "   - For each layer, initialise a weight matrix and a bias vector:\n",
    "      - The weight matrix (**W**) size is (input features size, output features size).\n",
    "      - The bias vector (**b**) size is (1, output features size).\n",
    "\n",
    "2. **Define Activation Functions for Hidden Layers:**\n",
    "   - Choose activation functions for hidden layers. Common choices include:\n",
    "      - [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)\n",
    "      - [Softmax](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "      - [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "3. **Define Forward Propagation Function:**\n",
    "   - Implement a function that computes the output of the neural network given an input by applying activation functions to the weighted sums of inputs at each layer.\n",
    "\n",
    "4. **Define Loss Function:**\n",
    "   - Choose a loss function that measures the distance between predicted and gold outputs. Common choices include mean squared error for regression tasks and cross-entropy loss for classification tasks.\n",
    "\n",
    "5. **Define Backward Propagation Function:**\n",
    "   - Implement a function that computes the gradients of the loss with respect to the weights and biases by applying the chain rule of calculus.\n",
    "\n",
    "6. **Define Training Loop:**\n",
    "   - Iterate through the training data multiple times, performing the following in each iteration:\n",
    "      - Forward pass to compute predictions.\n",
    "      - Compute loss using the chosen loss function.\n",
    "      - Backward pass to compute gradients.\n",
    "      - Update weights and biases using an optimisation algorithm (e.g., gradient descent).\n",
    "      - Repeat until the loss converges or a predefined number of iterations are reached.\n",
    "\n",
    "It's good practice to implement neural network architectures using the class data structure in Python for reusability. This allows training multiple instances with different hyperparameters and facilitates debugging in case of issues. Following this practice, below we implement our feedforward neural network as a class, incorporating the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c368ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetwork:\n",
    "    # Step 1: Initialise weights and biases\n",
    "    def __init__(self, input_size: int, hidden_size1: int, hidden_size2: int, output_size: int) -> None:\n",
    "        '''\n",
    "        Initialises the neural network with random weights and zero biases.\n",
    "\n",
    "        Args:\n",
    "          input_size (int): Number of features in the input data.\n",
    "          hidden_size1 (int): Number of neurons in the first hidden layer.\n",
    "          hidden_size2 (int): Number of neurons in the second hidden layer.\n",
    "          output_size (int): Number of classes in the output layer.\n",
    "        '''\n",
    "\n",
    "        # Layer 1\n",
    "        # Layer 1 weights matrix: (input_size, hidden_size1)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size1)\n",
    "        print(\"Weights layer 1 shape:\", self.W1.shape) # -> (1500, 64)\n",
    "        self.b1 = np.zeros((1, hidden_size1))\n",
    "\n",
    "        # Layer 2\n",
    "        # Layer 2 weights matrix: (hidden_size1, hidden_size2)\n",
    "        self.W2 = np.random.randn(hidden_size1, hidden_size2)\n",
    "        print(\"Weights layer 2 shape:\", self.W2.shape) # -> (64, 32)\n",
    "        self.b2 = np.zeros((1, hidden_size2))\n",
    "\n",
    "        # Layer 3 (Output layer)\n",
    "        # Layer 3 weights matrix: (hidden_size2, output_size)\n",
    "        self.W3 = np.random.randn(hidden_size2, output_size)\n",
    "        print(\"Weights layer 3 shape:\", self.W3.shape) # -> (32, 3)\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "\n",
    "\n",
    "    # Step 2: Activation Functions\n",
    "    # 2.1: activation functions\n",
    "    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "          Sigmoid function that gives us the predictions of the given samples.\n",
    "\n",
    "          Args:\n",
    "              Z (array): each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
    "\n",
    "          Returns:\n",
    "              np.ndarray: array containing a prediction for each sample.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def softmax(self, z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "          Softmax function that gives us the predictions of the given samples.\n",
    "\n",
    "          Args:\n",
    "              Z (array): each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
    "\n",
    "          Returns:\n",
    "              np.ndarray: array containing a prediction for each sample.\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    # 2.2: derivatives of activation functions\n",
    "    def d_sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "      return x * (1 - x)\n",
    "\n",
    "    # Step 3: Perform Forward propagation\n",
    "    def forward(self, X: np.ndarray, epoch: int=0) -> np.ndarray:\n",
    "        '''\n",
    "        Performs forward propagation for the neural network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with dimensions (num_samples, num_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output predictions after passing through the network's layers.\n",
    "        '''\n",
    "\n",
    "        ## INSERT YOUR CODE HERE ##\n",
    "\n",
    "        # Layer 1\n",
    "\n",
    "        # Compute Z for Layer 1 using input data and Layer 1 weights\n",
    "        # X: (num_train_samples, num_features) x weights layer 1: (input_size, hidden_size1) -> Z1: (num_train_samples, hidden_size1)\n",
    "        Z1 = np.dot(X, self.W1) + self.b1\n",
    "\n",
    "        # Compute the output of Layer 1 using the activation function of Layer 1 and Layer 1 Z\n",
    "        # Z1: (num_train_samples, hidden_size1) -> a1: (num_train_samples, hidden_size1)\n",
    "        self.a1 = self.sigmoid(Z1)\n",
    "\n",
    "        if epoch == 0:\n",
    "          print(\"Z1 shape (num_train_samples, hidden_size1):\", Z1.shape)  # -> (4500, 64)\n",
    "          print(\"a1 shape (num_train_samples, hidden_size1):\", self.a1.shape)  # -> (4500, 64)\n",
    "\n",
    "\n",
    "        # Layer 2\n",
    "\n",
    "        # Compute Z for Layer 2 using output of Layer 1 and Layer 2 weights\n",
    "        # a1: (num_train_samples, hidden_size1) x weights layer 2: (hidden_size1, hidden_size2) -> Z2: (num_train_samples, hidden_size2)\n",
    "        Z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "\n",
    "        # Compute the output of Layer 2 using the activation function of Layer 2 and Layer 2 Z\n",
    "        # Z2: (num_train_samples, hidden_size2) -> a2: (num_train_samples, hidden_size2)\n",
    "        self.a2 = self.sigmoid(Z2)\n",
    "\n",
    "        if epoch == 0:\n",
    "          print(\"Z2 shape (num_train_samples, hidden_size2):\", Z2.shape)  # -> (4500, 32)\n",
    "          print(\"a2 shape (num_train_samples, hidden_size2):\", self.a2.shape)  # -> (4500, 32)\n",
    "\n",
    "\n",
    "        # Layer 3 (Output Layer)\n",
    "\n",
    "        # Compute Z for Layer 3 using output of Layer 2 and Layer 3 weights\n",
    "        # a2: (num_train_samples, hidden_size2) x weights layer 3: (hidden_size2, output_size) -> Z3: (num_train_samples, output_size)\n",
    "        Z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "\n",
    "        # Compute the output of Layer 3 (output of the network) using the activation function of Layer 3 and Layer 3 Z\n",
    "        # Z3: (num_train_samples, output_size) -> a3: (num_train_samples, output_size)\n",
    "        self.a3 = self.softmax(Z3)\n",
    "\n",
    "        if epoch == 0:\n",
    "          print(\"Z3 shape (num_train_samples, output_size):\", Z3.shape)  # -> (4500, 3)\n",
    "          print(\"a3 shape (num_train_samples, output_size):\", self.a3.shape)  # -> (4500, 3)\n",
    "\n",
    "        ## END OF YOUR CODE ##\n",
    "\n",
    "        return self.a3\n",
    "\n",
    "\n",
    "    # Step 4: Cross-Entropy Loss Calculation\n",
    "    def cross_entropy_loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        '''\n",
    "        Computes the cross-entropy loss between the true labels and predicted probabilities.\n",
    "\n",
    "        Args:\n",
    "          y_true (numpy.ndarray): True class labels with dimensions (num_samples,).\n",
    "          y_pred (numpy.ndarray): Predicted probabilities with dimensions (num_samples, num_classes).\n",
    "\n",
    "        Returns:\n",
    "          float: Cross-entropy loss.\n",
    "        '''\n",
    "        num_samples = len(y_true)\n",
    "        loss = -np.sum(np.log(y_pred[np.arange(num_samples), y_true])) / num_samples\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # Step 6: Perform Backward propagation\n",
    "    def backward(self, X: np.ndarray, y_true: np.ndarray, learning_rate: float=0.01, epoch: int=1) -> None:\n",
    "        '''\n",
    "        Computes the cross-entropy loss between the true labels and predicted probabilities.\n",
    "\n",
    "        Args:\n",
    "          X (numpy.ndarray): Samples in the dataset.\n",
    "          y_true (numpy.ndarray): True class labels with dimensions (num_samples,).\n",
    "          learning_rate (float): Learning rate used to update weights. Deafulto to 0.01.\n",
    "          epoch (int): current epoch during training, used to print matrix weights only for first epoch. Default to 1.\n",
    "        '''\n",
    "\n",
    "        num_samples = len(X)\n",
    "\n",
    "        # 5.1 Compute Gradients for Layer 3 (Output Layer) (d_a3, d_W3, d_b3):\n",
    "        '''\n",
    "        d_a3: Calculate the error at the output layer by subtracting 1\n",
    "         from the predicted probabilities corresponding to the true class.\n",
    "\n",
    "        d_W3: Compute the gradient of the loss with respect to the weights connecting\n",
    "         the third layer (output layer) to the second layer.\n",
    "\n",
    "        d_b3: Compute the gradient of the loss with respect to the biases of the output layer.\n",
    "        '''\n",
    "        d_a3 = self.a3.copy()\n",
    "        d_a3[np.arange(num_samples), y_true] -= 1\n",
    "        d_L3 = d_a3/ num_samples\n",
    "\n",
    "        d_W3 = np.dot(self.a2.T, d_L3)\n",
    "        d_b3 = np.sum(d_L3, axis=0, keepdims=True)\n",
    "        if epoch == 0:\n",
    "          print(\"d_a3 shape (num_train_samples, output_size):\", d_a3.shape)  # -> (4500, 3)\n",
    "          print(\"d_L3 shape (num_train_samples, output_size):\", d_L3.shape)  # -> (4500, 3)\n",
    "          print(\"d_W3 shape (hidden_size2, output_size):\", d_W3.shape)  # -> (32, 3)\n",
    "          print(\"d_b3 shape (1, output_size):\", d_b3.shape)  # -> (1, 3)\n",
    "\n",
    "        # 5.2 Compute Gradients for Layer 2 (d_a2, d_W2, d_b2):\n",
    "        '''\n",
    "        d_a2: Calculate the error at the hidden layer .\n",
    "\n",
    "        d_W2: Compute the gradient of the layer 3 output with respect to the weights connecting\n",
    "         the second layer (hidden layer) to the first layer (input layer).\n",
    "\n",
    "        d_b2: Compute the gradient of the layer 3 output with respect to the biases of the hidden layer.\n",
    "        '''\n",
    "        d_a2 = np.dot(d_L3, self.W3.T)\n",
    "        d_L2 = d_a2 * (self.d_sigmoid(self.a2))\n",
    "\n",
    "        d_W2 = np.dot(self.a1.T, d_L2)\n",
    "        d_b2 = np.sum(d_L2, axis=0, keepdims=True)\n",
    "        if epoch == 0:\n",
    "          print(\"d_a2 shape (num_train_samples, hidden_size2):\", d_a2.shape)  # -> (4500, 32)\n",
    "          print(\"d_L2 shape (num_train_samples, hidden_size2):\", d_L2.shape)  # -> (4500, 32)\n",
    "          print(\"d_W2 shape (hidden_size1, hidden_size2):\", d_W2.shape)  # -> (64, 32)\n",
    "          print(\"d_b2 shape (1, hidden_size2):\", d_b2.shape)  # -> (1, 32)\n",
    "\n",
    "        # 5.3 Compute Gradients for Layer 1 (d_a1, d_W1, d_b1):\n",
    "        d_a1 = np.dot(d_L2, self.W2.T)\n",
    "        d_L1 = d_a1 * (self.d_sigmoid(self.a1))\n",
    "\n",
    "        d_W1 = np.dot(X.T, d_L1)\n",
    "        d_b1 = np.sum(d_L1, axis=0, keepdims=True)\n",
    "        if epoch == 0:\n",
    "          print(\"d_a1 shape (num_train_samples, hidden_size1):\", d_a1.shape)  # -> (4500, 64)\n",
    "          print(\"d_L1 shape (num_train_samples, hidden_size1):\", d_L1.shape)  # -> (4500, 64)\n",
    "          print(\"d_W1 shape (input_size, hidden_size2):\", d_W1.shape)  # -> (1500, 64)\n",
    "          print(\"d_b1 shape (1, input_size):\", d_b1.shape)  # -> (1, 64)\n",
    "\n",
    "        # 5.4 Update weights and biases\n",
    "        self.W1 -= learning_rate * d_W1\n",
    "        self.b1 -= learning_rate * d_b1\n",
    "        self.W2 -= learning_rate * d_W2\n",
    "        self.b2 -= learning_rate * d_b2\n",
    "        self.W3 -= learning_rate * d_W3\n",
    "        self.b3 -= learning_rate * d_b3\n",
    "\n",
    "    # Step 6: Training Loop\n",
    "    def train(self, X: np.ndarray, y_true: np.ndarray, epochs: int=100, learning_rate: float=0.01) -> None:\n",
    "        '''\n",
    "        Trains the neural network using the specified training data.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with dimensions (num_samples, num_features).\n",
    "            y_true (numpy.ndarray): True class labels with dimensions (num_samples,).\n",
    "            epochs (int): Number of training epochs. Default to 100.\n",
    "            learning_rate (float): Learning rate for weight updates. Default to 0.01.\n",
    "        '''\n",
    "        self.losses = []\n",
    "\n",
    "        # tqdm is a function that given a list, the total number of iterations and a description,\n",
    "        # returns a progressbar showing the time execution\n",
    "        # pbar will be updated to include the loss at each epoch\n",
    "        pbar = tqdm(range(epochs), total=epochs, desc=\"Training\")\n",
    "        for epoch in pbar:\n",
    "              # 6.1 Forward pass\n",
    "              y_pred = self.forward(X, epoch)\n",
    "\n",
    "              # 6.2 Compute loss\n",
    "              loss = self.cross_entropy_loss(y_true, y_pred)\n",
    "              self.losses.append(loss)\n",
    "              # updated the progressbar to include the loss of this epoch\n",
    "              pbar.set_description(f'Training - Epoch {epoch + 1}/{epochs}, Loss: {loss:0.03f}') #0.03f to display the first decimal points\n",
    "\n",
    "              # 6.3 Backward pass\n",
    "              self.backward(X, y_true, learning_rate, epoch)\n",
    "\n",
    "\n",
    "    def show_learning_curve(self) -> None:\n",
    "        '''\n",
    "        Displays the learning curve (loss curve) during training.\n",
    "        '''\n",
    "        plt.plot(self.losses)\n",
    "        plt.xlabel('Epochs') # x axis label\n",
    "        plt.ylabel('Cross-Entropy Loss') # y axis label\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def test(self, X: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Calculates the predictions for the given samples.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with dimensions (num_samples, num_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output predictions after passing through the network's layers.\n",
    "        '''\n",
    "        y_pred = self.forward(X)\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
