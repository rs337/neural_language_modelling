{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkidVBaGmyXp"
      },
      "source": [
        "# CA6011 Deep Learning for NLP: Week 3 Lab Part 3 -- Hugging Face\n",
        "\n",
        "In this part of the lab the aim is to get a first feel for how to create and apply NLP systems on Hugging Face.\n",
        "\n",
        "We'll start with the quickest way to get an NLP system up and running, in one single step using `pipeline`, for sentiment analysis, prompted text generation and summarisation. See the pipeline documentation for details: https://huggingface.co/docs/transformers/main_classes/pipelines\n",
        "\n",
        "Then we'll take a look at what happens behind the scenes, and code up the system in several steps equivalent to `pipeline`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4U0i15Hn5zQ"
      },
      "source": [
        "First up is a **sentiment analysis** system which we create with the simple line:\n",
        "\n",
        "    classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "Note that here we simply state the *task*, nothing else, and everything needed to solve the task is supplied via defaults. Once created, we can simply call `classifier` with any text or list of texts as the argument and get a (list of) class label(s) and score(s) returned.\n",
        "\n",
        "The range of tasks that can be addressed in this way can be found here: https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.task\n",
        "\n",
        "The default sentiment analysis model (`distilbert-base-uncased-finetuned-sst-2-english`) used is shown in the output below (we'll be using it explicitly later). It classifies input texts into one of two output classes (POSITIVE, NEGATIVE).\n",
        "\n",
        "Note the model naming convention: the name of the raw model (`distilbert base`), followed by an indication that it is a finetuned version of the raw model (`finetuned`), followed by the name of the dataset it was finetuned on (`sst 2 english`).\n",
        "\n",
        "The actual system output is shown last and identifies the class predicted (POSITIVE) and the probability assigned to it (0.99985)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "83Nb30ASmRXD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998509883880615}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "output = classifier(\"A smart, solidly crafted procedural that's anchored in family drama, Anatomy of a Fall finds star Sandra Hüller and director/co-writer Justine Triet operating at peak power.\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvBv0fOqpDB0"
      },
      "source": [
        "Getting the class label `POSITIVE` and a high probability score as the output returned is exactly what we would expect, given the glowing film review snippet we gave the system to classify.\n",
        "\n",
        "Next we're creating and using a **prompted generation** system in the same way as the sentiment analyser above. Except that this time we're not using the default text-generation model; instead we're specifying `distilgpt2` as the model to be used.\n",
        "\n",
        "Any model on the Hugging Face hub can in principle be used in this way in a pipeline: https://huggingface.co/models\n",
        "\n",
        "Note that if the model defines the task, then the first task argument can be omitted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rMSagVFvyFf3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"A smart, solidly crafted procedural that's anchored in family drama, Anatomy of a Fall finds Âs humor and charm.\\n\\n\\n\\n\\nThe script is produced by David B. B. DeGrasse Tyson.\\nThe script is produced by David B. DeGrasse Tyson.\"}]\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "output = generator(\"A smart, solidly crafted procedural that's anchored in family drama, Anatomy of a Fall finds \",truncation=True, max_length=100, num_return_sequences=1)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuo8-cEs32cL"
      },
      "source": [
        "The continuation we get back kind of reads like a review but likely is nonsensical in places. Note that if you run the generator multiple times you will get different continuations.\n",
        "\n",
        "You can also ask for continuations of different length by setting `max_length` and get multiple continuations by setting `num_return_sequences`, the last two arguments in the `generator` call.\n",
        "\n",
        "Next we're going to create a **summarizer**. This time we're specifying the model and the tokenizer we want to use in the arguments to the summarizer call, but we're moreover first loading their weights explicitly with the `from_pretrained` method.\n",
        "\n",
        "For this we need to import matching model and tokenizer architectures to load the weights into (the first line of the code below). Re loading pretrained model checkpoints see here: https://huggingface.co/learn/nlp-course/chapter4/2#using-pretrained-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9s-tiJMa47gq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'summary_text': ' Anatomy of a Fall is an uncommonly perceptive and thought-provoking procedural procedural . The movie transpires in France and works using the rules of French jurisprudence . Director Justine Triet rejects a facile omniscient representation of the death-scene at any point .'}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
        "\n",
        "text = \"\"\"Anatomy of a Fall is an uncommonly perceptive and thought-provoking procedural.\n",
        "          Because the movie transpires in France and works using the rules of French jurisprudence,\n",
        "          it is better able to address questions of truth than a U.S.-based iteration of the same story\n",
        "          would be able to do. (The case would never make it in front of a judge in an American court.)\n",
        "          By focusing on the narrative that emerges during a trial rather than the events of what happened\n",
        "          at the chalet shared by wife Sandra Voyter (Sandra Huller), husband Samuel Maleski (Samuel Theis),\n",
        "          and their son, Daniel (Milo Machado Graner), Anatomy of a Fall can ponder the unknowability of any\n",
        "          objective truth. It’s another facet of the Rashomon prism.\n",
        "          Recognizing that images captured by the camera represent something concrete, director Justine Triet\n",
        "          is careful about deciding what to show on-screen. The instance of death is never depicted; we see\n",
        "          precursor moments and are by Daniel’s side when he discovers the body but the minutia surrounding\n",
        "          the actual death is left for the lawyers to argue. And, because she wants to emphasize the elusive\n",
        "          nature of an objective truth, Triet rejects a facile omniscient representation of the death-scene at any point.\"\"\"\n",
        "\n",
        "task = \"summarization\"\n",
        "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "summariser = pipeline(task, model=model, tokenizer=tokenizer)\n",
        "\n",
        "output = summariser(text, num_return_sequences=1)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdSnL8-142Ip"
      },
      "source": [
        "The summary produced is relevant, but relies on extraction rather than abstraction. Admittedly it's a tough review to summarise!\n",
        "\n",
        "Running the above will give the same output every time, but we can ask for alternatives by increasing `num_return_sequences`.\n",
        "\n",
        "Let's take a closer look at what the tokenizer does, using the same tokenizer as in the summarization example above, but using a shorter text so we can more easily see what's going on.\n",
        "\n",
        "First we'll look at what the `tokenizer` call returns which is the token IDs and the attention mask indicating which tokens to ignore (none in this example). Note that the list of token IDs contains two extra token IDs: the start-of-sequence ID 0 and the end-of-sequence ID 2.\n",
        "\n",
        "Next we'll look at the intermediate steps of converting the input text into a token representation, and the latter into token IDs (this time without delimiting tokens). Finally we decode the list of IDs back into a word sequence which should give us the original text back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vxrp3SJ--CaB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenizer(text) output:  {'input_ids': [0, 4688, 415, 13604, 9, 10, 9197, 16, 41, 18186, 352, 228, 42579, 8, 802, 12, 13138, 14805, 24126, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "tokens:  ['An', 'at', 'omy', 'Ġof', 'Ġa', 'ĠFall', 'Ġis', 'Ġan', 'Ġuncommon', 'ly', 'Ġper', 'ceptive', 'Ġand', 'Ġthought', '-', 'prov', 'oking', 'Ġprocedural', '.']\n",
            "ids:  [4688, 415, 13604, 9, 10, 9197, 16, 41, 18186, 352, 228, 42579, 8, 802, 12, 13138, 14805, 24126, 4]\n",
            "decoded ids:  Anatomy of a Fall is an uncommonly perceptive and thought-provoking procedural.\n"
          ]
        }
      ],
      "source": [
        "text = 'Anatomy of a Fall is an uncommonly perceptive and thought-provoking procedural.'\n",
        "\n",
        "output = tokenizer(text)\n",
        "print(\"tokenizer(text) output: \", output)\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"tokens: \", tokens)\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"ids: \", ids)\n",
        "\n",
        "decoded_ids = tokenizer.decode(ids)\n",
        "print(\"decoded ids: \", decoded_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgZto3FyA9ZL"
      },
      "source": [
        "And indeed we do get the original word sequence back. Decoding a shortened sequence will work too (or any other sequence for that matter):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hWxzvsvpBQ51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Anatomy of a Fall is uncommonly perceptive.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode([4688, 415, 13604, 9, 10, 9197, 16, 18186, 352, 228, 42579, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63KutOx7BQcS"
      },
      "source": [
        "In our final example, we're returning to **sentiment analysis**, but this time we're not using `pipeline`, instead we're running `model` directly to obtain logits, which we then pass to `softmax` to obtain probabilities, and finally to `argmax` to determine the winning class.\n",
        "\n",
        "Note that this time we're also moving to multiple input texts, processed at the same time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3RuxuvvtIvkL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized inputs using separate steps:\n",
            " {'input_ids': tensor([[  101, 13336,  1997,  1037,  2991,  2003,  2019, 13191,  2135,  2566,\n",
            "         28687,  1998,  2245,  1011,  4013, 22776, 24508,  1012,   102,     0,\n",
            "             0,     0,     0],\n",
            "        [  101, 13336,  1997,  1037,  2991,  2064, 29211,  1996,  4895,  2243,\n",
            "         19779,  8010,  1997,  2151,  7863,  3606,  1012,   102,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101, 13012,  3388, 19164,  1037,  6904,  6895,  2571, 18168,  8977,\n",
            "         23402,  3372,  6630,  1997,  1996,  2331,  1011,  3496,  2012,  2151,\n",
            "          2391,  1012,   102],\n",
            "        [  101,  2009,  1521,  1055,  2178,  2227,  2102,  1997,  1996, 23438,\n",
            "         19506,  2078, 26113,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "Logits using separate steps:\n",
            " tensor([[-4.0490,  4.2323],\n",
            "        [ 1.1051, -0.8726],\n",
            "        [ 2.9422, -2.4065],\n",
            "        [ 1.0962, -0.8520]])\n",
            "Scores using separate steps:\n",
            " tensor([[2.5314e-04, 9.9975e-01],\n",
            "        [8.7844e-01, 1.2156e-01],\n",
            "        [9.9527e-01, 4.7316e-03],\n",
            "        [8.7525e-01, 1.2475e-01]])\n",
            "Labels using separate steps:\n",
            " tensor([1, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "inputs = [\"Anatomy of a Fall is an uncommonly perceptive and thought-provoking procedural.\",\n",
        "         \"Anatomy of a Fall can ponder the unknowability of any objective truth.\",\n",
        "         \"Triet rejects a facile omniscient representation of the death-scene at any point.\",\n",
        "         \"It’s another facet of the Rashomon prism.\"]\n",
        "\n",
        "tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "print(\"Tokenized inputs using separate steps:\\n\", tokenized_inputs)\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = model(**tokenized_inputs) # unpack dictionary to use as argument values\n",
        "  print(\"Logits using separate steps:\\n\", output.logits)\n",
        "  scores = F.softmax(output.logits, dim=1)\n",
        "  print(\"Scores using separate steps:\\n\", scores)\n",
        "  labels = torch.argmax(scores, dim=1)\n",
        "  print(\"Labels using separate steps:\\n\", labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSlum0TZJIGd"
      },
      "source": [
        "This gives us in order the four tokenized inputs and their attention masks, the corresponding logits produced by the model, the probabilities that the logits map to, and finally the winning class labels.\n",
        "\n",
        "Now let's check that a pipeline consisting of the same model and tokenizer would have produced the same result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7hz-X_XzJKml"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels and scores using pipeline:  [{'label': 'POSITIVE', 'score': 0.9997468590736389}, {'label': 'NEGATIVE', 'score': 0.8784357309341431}, {'label': 'NEGATIVE', 'score': 0.9952684044837952}, {'label': 'NEGATIVE', 'score': 0.8752526044845581}]\n"
          ]
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "outputs = classifier(inputs)\n",
        "print(\"Labels and scores using pipeline: \", outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzQVQFysJNW8"
      },
      "source": [
        "If you compare the probabilities with those previously produced by the separate steps above, you'll find they're the same.\n",
        "\n",
        "Now it's over to you. In the cell below, create a pipeline for a task of your choosing, first just specifying the task, then also the model and tokenizer, finally replacing the pipeline with steps in the same way we did above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid character '│' (U+2502) (3295246654.py, line 2)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m│                            │                            │                            │\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '│' (U+2502)\n"
          ]
        }
      ],
      "source": [
        "Input Text                    Tokenizer                     Model                      Tokenizer\n",
        "     │                            │                            │                            │\n",
        "     ▼                            ▼                            ▼                            ▼\n",
        "\"Hello world\"  ──tokenize──►  [101, 7592, 2088, 102]  ──forward──►  [4532, 234, ...]  ──decode──►  \"Dia duit\"\n",
        "                              (numbers the model                   (output token IDs)\n",
        "                               can process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5p0YJyBQKFXi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'translation_text': 'Shiúil muid go dtí na siopaí'}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'translation_text': 'Is fuath liom Béarla ...'}]\n",
            "Tokenized input:\n",
            " {'input_ids': tensor([[ 1015, 23317,     8,     3, 19997,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])} ['▁We', '▁walked', '▁to', '▁the', '▁shops']\n",
            "Output token IDs:\n",
            " tensor([[56682,  3342,  3151,  1466,    25,   303,    14, 14559,     0]],\n",
            "       device='mps:0')\n",
            "Translated text:\n",
            " Shiúil muid go dtí na siopaí\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "# Step 1: Create a pipeline for a task of your choosing, then test it with some inputs\n",
        "\n",
        "## INSERT YOUR CODE HERE ##\n",
        "translate_this = \"We walked to the shops\"\n",
        "translate = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ga\")\n",
        "output = translate(translate_this)\n",
        "print(output)\n",
        "## END OF YOUR CODE ##\n",
        "\n",
        "\n",
        "# Step 2: Now specify the model and tokenizer you wish to use in the pipeline\n",
        "\n",
        "## INSERT YOUR CODE HERE ##\n",
        "task = \"translation\"\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-ga\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "translater = pipeline(task, model=model, tokenizer=tokenizer)\n",
        "out2 = translater(\"I hate english ...\")\n",
        "print(out2)\n",
        "## END OF YOUR CODE ##\n",
        "\n",
        "\n",
        "# Step 3: Finally, use separate steps equivalent to your pipeline with specified model and tokenizer\n",
        "# to produce outputs for the same inputs as in Step 1, and check that the outputs are the same\n",
        "\n",
        "## INSERT YOUR CODE HERE ##\n",
        "tokenized_input = tokenizer(translate_this, return_tensors=\"pt\")\n",
        "tokens = tokenizer.tokenize(translate_this)\n",
        "print(\"Tokenized input:\\n\", tokenized_input, tokens)\n",
        "\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "tokenized_input = tokenizer(translate_this, return_tensors=\"pt\")\n",
        "tokenized_input = {k: v.to(device) for k, v in tokenized_input.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(**tokenized_input)\n",
        "    print(\"Output token IDs:\\n\", output_ids)\n",
        "\n",
        "translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"Translated text:\\n\", translated_text)\n",
        "## END OF YOUR CODE ##"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
