{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ranSpOffE3VY"
      },
      "source": [
        "# CA6011 Deep Learning for NLP: Week 4 Lab -- Recurrent Neural Networks\n",
        "\n",
        "**Recurrent Neural Networks** (RNNs) are another type of artificial neural networks designed to process sequential data by maintaining a memory of previous inputs. Unlike feedforward neural networks (Week 2), where information flows only in one direction (from input to output), RNNs have connections that allow information to persist over time and influence the processing of subsequent inputs. This makes RNNs well-suited for tasks involving sequential data, such as speech recognition and handwriting recognition.\n",
        "\n",
        "The success of Recurrent Neural Networks (RNNs) in modeling data hinges on their ability to maintain a form of short-term memory which is updated at each time step as new input is processed. Through the simple mechanism of copying the preceding hidden state and concatenating it to current inputs, this memory captures information about past inputs, allowing previous inputs to shape future outputs at each time step. See the lecture slides and references for details.\n",
        "\n",
        "In this lab, you will code, more or less from scratch, a simple Recurrent Neural Network (sometimes called an Elman Network, see original paper in Week 4 Lab folder) and apply it to a sentiment classification task.\n",
        "\n",
        "Here are two examples of the sentiment classification data we will use (for  more information see the Dataset Section (2.1) below):\n",
        " ```\n",
        "   text: 'this is very good', label: True,\n",
        "   text: 'i am sad right now', label: False,\n",
        " ```\n",
        "\n",
        "As before, we provide a coding framework with a lot of the code in place, for you to add to.\n",
        "\n",
        "The remainder of the lab (for which we don't provide solutions) is for you to complete on your own, and must be submitted for assessment for Week 4 of this module (see handbook).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px72Z8CKj3nO"
      },
      "source": [
        "Troughout this notebook we are going to refer to the lecture slides, and also [Speech and Language Processing (3rd ed. draft) Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/), this week specifically Sections 13.1-13.3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "A8M-G8P2Ej5A"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline allows for displaying plots directly in the Jupyter notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Pytorch is Deep learning framework\n",
        "import torch\n",
        "\n",
        "# NumPy is a library for numerical computations, with support for arrays and matrices\n",
        "import numpy as np\n",
        "\n",
        "# tqdm is used for creating progress bars to track the progress of for loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Matplotlib is a plotting library, and we use it for tracking the loss values across iterations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# IPython.display allows clearing the output in a Jupyter notebook cell\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# use F to import Tanh and softmax activation function\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Importing the random module for generating random numbers/orders\n",
        "import random\n",
        "\n",
        "# Importing the list data type for storing collections of items.\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl9L4GOoUMRf"
      },
      "source": [
        "\n",
        "## 1 Recurrent Neural Network Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avU1okK2cYBQ"
      },
      "source": [
        "### 1.1 Activation Functions definition and derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCbY_RSeda0O"
      },
      "source": [
        "In our implementation, we'll use Tanh and Softmax as activation functions:\n",
        "- **Tanh Function:**\n",
        "\n",
        "  $ \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
        "\n",
        "  *Derivative with respect to **z**:*\n",
        "  $\\frac{d}{dz} \\tanh(z) = (1 - \\tanh(z)^2)$\n",
        "\n",
        "- **Softmax Function:**\n",
        "\n",
        "  $\\text{softmax}(z_k) = \\frac{e^{z_k}}{\\sum_{j=1}^{k} e^{z_j}}$ , Where k is the number of classes\n",
        "\n",
        "  *Derivative with respect to **z**$_k$:*\n",
        "  $\\frac{\\partial}{\\partial z_k} \\text{softmax}(z_k) = \\text{softmax}(z_k) \\cdot (1 - \\text{softmax}(z_k))$\n",
        "\n",
        "**Note:** As discussed in previous lectures and labs, we require the derivatives of all function involved in forward propagation. These derivatives indicate how each weight and bias needs to be changed to minimise the loss function, forming the basis for the backward propagation loop for updating the weights and biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ39eyqhcwcP"
      },
      "source": [
        "### 1.2 Backward Propagation definition\n",
        "\n",
        "As explained in Week 2, the goal of backward propagation is to calculate the change to be made to each weight and bias on the basis of the loss we receive from the forward propagation. We need to change the weights and biases individually, because the loss is a function of all the weights and biases (i.e. many variables), so we use partial derivatives with respect to each weight and bias. Sed previous lectures for details.\n",
        "\n",
        "In RNNs, backpropagation works pretty much the same way, with one small modification: the backward pass has to involve not just the current hidden state, but all previous hidden states in the current input sequence. For algorithmic simplicity, we `unroll' the network across time to obtain a single network that can be treated as a FNN for training and inference purposes. When backpropagation works like this it's called Backpropagetion through time (BPTT). See lectures and Jurafsky & Martin, Section 13.1 for details.\n",
        "\n",
        "[Backpropagation through time (BPTT)](https://en.wikipedia.org/wiki/Backpropagation_through_time) is a gradient-based technique used to train Recurrent Neural Networks (RNNs). In RNNs, while weights and biases are shared across all time steps, it's crucial to track the changes in weights and biases corresponding to the received input at each time step. This tracking is accomplished by unrolling the recurrent connections, as shown in the lecture slides on Slide 13, replicating the weights shared across time steps, and feeding in the inputs received at time t, for each time step t."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPfeAKUoguj-"
      },
      "source": [
        "In the **forward propagation** step for Elman RNNs, we compute the following operations and apply them recurrently for all time steps:\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "h_i &= g(Uh_{i-1} + Wx_i) \\tag{1} \\\\\n",
        "y_i &= f(Vh_i) \\tag{2}\n",
        "\\end{align*}\n",
        "\n",
        "Where:\n",
        "- $x_i$ is the input at the current time step.\n",
        "- $y_i$ is the output at the current time step.\n",
        "- $h_i$ is the hidden state at the current time step.\n",
        "- $h_{i-1}$ is the hidden state at the previous time step.\n",
        "- $g$ is the hidden-layer activation function, Tanh usually\n",
        "- $f$ is the activation function at the output layer, in our implementation we use softmax.\n",
        "- $ U,  W, V$ are learnable weights matrices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXmwNNF8h9KP"
      },
      "source": [
        "So, in the backward propagation step, we carry out the following backward differentiation computations (refer to Week 2 lecture and lab):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk6zmYKydYJZ"
      },
      "source": [
        "**Step 1: Compute Loss**\n",
        "\n",
        "For a single time step $i$, let $L_i$ be the cross-entropy loss function:\n",
        "\n",
        "$L_i = -\\sum_{k} \\hat{y}_{i,k} \\log(y_{i,k})$\n",
        "\n",
        "where $ \\hat{y}_{i,k} $ is the true label for class $k$ and $ y_{i,k}$ is the predicted probability for class *k*.\n",
        "\n",
        "**Step 2: Backpropagate Through Output Layer**\n",
        "\n",
        "We compute the gradient of the loss with respect to the output prediction $ y_i$, denoted as $ \\frac{\\partial L_i}{\\partial y_i} $, using the derivative of the cross-entropy loss function:\n",
        "$\n",
        "\\frac{\\partial L_i}{\\partial y_{i,k}} = \\frac{-\\hat{y}_{i,k}}{y_{i,k}}\n",
        "$\n",
        "\n",
        "Using this gradient, we compute the gradients of the weights $ V$ connecting the hidden state $ h_i$ to the output layer:\n",
        "$\n",
        "\\frac{\\partial L_i}{\\partial V} = \\frac{\\partial L_i}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial V} = \\frac{\\partial L_i}{\\partial y_i} \\cdot h_i\n",
        "$\n",
        "\n",
        "**Step 3: Backpropagate Through Hidden Layer**\n",
        "\n",
        "We backpropagate the gradients through the output layer to the hidden layer. We first compute the gradient of the loss with respect to the hidden state $ h_i $:\n",
        "$\n",
        "\\frac{\\partial L_i}{\\partial h_i} = \\frac{\\partial L_i}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial h_i} = V^T \\cdot \\frac{\\partial L_i}{\\partial y_i}\n",
        "$\n",
        "\n",
        "Then, we compute the gradients of the weights $ U $ and $ W $ connecting the previous hidden state $ h_{i-1} $ and the input $ x_i $ to the current hidden state $ h_i $, respectively:\n",
        "\n",
        "$\n",
        "\\frac{\\partial L_i}{\\partial U} = \\frac{\\partial L_i}{\\partial h_i} \\cdot \\frac{\\partial h_i}{\\partial U} = \\frac{\\partial L_i}{\\partial h_i} \\cdot h_{i-1}\n",
        "$\n",
        "\n",
        "$\n",
        "\\frac{\\partial L_i}{\\partial W} = \\frac{\\partial L_i}{\\partial h_i} \\cdot \\frac{\\partial h_i}{\\partial W} = \\frac{\\partial L_i}{\\partial h_i} \\cdot x_i\n",
        "$\n",
        "\n",
        "**Step 4: Update Weights**\n",
        "\n",
        "Finally, we update the weights $ U $, $ W $, and $ V $ using the computed gradients and a learning rate:\n",
        "\n",
        "$$\n",
        "U := U - \\text{learning\\_rate} \\cdot \\frac{\\partial L_i}{\\partial U}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W := W - \\text{learning\\_rate} \\cdot \\frac{\\partial L_i}{\\partial W}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V := V - \\text{learning\\_rate} \\cdot \\frac{\\partial L_i}{\\partial V}\n",
        "$$\n",
        "\n",
        "These equations represent the gradient updates for each parameter in the Elman-style RNN during the backpropagation through time process. They allow the network to adjust its weights to minimise the loss and improve its predictions over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lEs6n7DcZOT"
      },
      "source": [
        "### 1.3 Network definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z01H-lJfb-uA"
      },
      "source": [
        "To create an Elman Recurrent Neural Network (RNN) and train it, we follow the same steps as in the Week 2 Lab (FNN implementation).\n",
        "However, as we have become familiar with these implementations, we won't build everything from scratch, especially for activation functions. Instead, we will utilise their NumPy implementation, as we are already familiar with how to implement and use them.\n",
        "\n",
        "Here's a quick reminder of those steps:\n",
        "\n",
        "1. **Initialise Layers:**\n",
        "   - For each layer, initialise a weight matrix:\n",
        "      - The weight matrix **W**: here we need three weight matrices **W, U, V** as per equations 1 and 2 above.\n",
        "      - Biases: we will add biases to the **U** and **V** weight matrices.\n",
        "\n",
        "2. **Define Activation Functions for hidden state and output:**\n",
        "   - Common choices for RNN include:\n",
        "      - [Tanh](https://en.wikipedia.org/wiki/Hyperbolic_functions): use torch.tanh() instead of implementing the activation function from scratch\n",
        "      - [Softmax](https://en.wikipedia.org/wiki/Softmax_function): we use a softmax from torch.nn.functional, take a look at the import cell, we imported \"torch.nn.functional as F\", to use softmax we will do (F.softmax()).\n",
        "\n",
        "3. **Define Forward Propagation Function:**\n",
        "   - Implement a function that computes the output of the neural network given an input and initial hidden state by applying the previous equations 1 and 2.\n",
        "\n",
        "\n",
        "4. **Define Backward Propagation Function:**\n",
        "   - Implement a function that computes the gradients of the loss with respect to the weights and biases by applying the chain rule of calculus across time steps.\n",
        "\n",
        "5. **Define Training Loop:**\n",
        "   - Iterate through the training data multiple times, performing the following in each iteration:\n",
        "      - Forward pass to compute predictions.\n",
        "      - Compute loss using the chosen loss function.\n",
        "      - Backward pass to compute gradients.\n",
        "      - Update weights and biases.\n",
        "      - Repeat until the loss converges or a predefined number of iterations are reached.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Note:** In this lab, we will use the @ operator or matmul function (i.e., matrix multiplication). As you are now well-versed in matrix multiplication (see Week 2 Lab), we will prefer to use this operator. Essentially, @ and np.dot behave similarly for 2D matrix multiplication, but @ is preferred for its readability and ease of debugging, as it's a bit challenging to implement RNNs compared to FFNs. The code may become messy with RNN implementations, making @ the preferred choice. For more information on the differences between these operators, you can refer to [this link](https://stackoverflow.com/questions/34142485/difference-between-numpy-dot-and-python-3-5-matrix-multiplication).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiQki3K-OI8S"
      },
      "outputs": [],
      "source": [
        "class ElmanRNN:\n",
        "\n",
        "  # Step 1 Initialises the Elman-style RNN Architecture\n",
        "  def __init__(self, input_size, output_size, hidden_size=64):\n",
        "    '''\n",
        "        Initialises the Elman-style RNN with random weights and zero biases.\n",
        "\n",
        "        Args:\n",
        "          input_size (int): Number of features in the input data.\n",
        "          hidden_size (int): Number of neurons in the hidden layer.\n",
        "          output_size (int): Number of classes in the output layer.\n",
        "    '''\n",
        "    # Weights\n",
        "    '''\n",
        "    randn returns a sample from the standard normal distribution.\n",
        "    We divide by 1000 (just a value that seems to work with our setup).\n",
        "    The goal is to have a better initialisation.\n",
        "    The ultimate objective of different initialisation techniques is\n",
        "    to ensure that our weights are neither too small nor too large,\n",
        "    as this has a significant impact on training dynamics and neural network stability.\n",
        "    '''\n",
        "    self.hidden_size = hidden_size #make hidden size accessable to other functions in the class\n",
        "\n",
        "    # Weight matrices\n",
        "    self.U = torch.randn(hidden_size, hidden_size) / 1000\n",
        "    self.W = torch.randn(hidden_size, input_size) / 1000\n",
        "    self.V = torch.randn(output_size, hidden_size) / 1000\n",
        "\n",
        "    # Biases\n",
        "    self.BU = torch.zeros(hidden_size, 1)\n",
        "    self.BV = torch.zeros(output_size, 1)\n",
        "\n",
        "  # Step 2: For activation functions, we will use their torch implementation instead of from-scratch\n",
        "\n",
        "  # Step 3: Perform Forward propagation\n",
        "  def forward(self, inputs, verbose:bool=False):\n",
        "    '''\n",
        "    Perform a forward pass of the RNN using the given inputs.\n",
        "    Returns the final output and hidden state.\n",
        "    - inputs is a list of arrays of one hot vectors with shape (input_size, 1) for each word in the sentence.\n",
        "    - verbose: it's a common flag in programming, enable if you want to print meta information about your function.\n",
        "    '''\n",
        "    ## INSERT YOUR CODE HERE ##\n",
        "    #initialise the hidden state to zero -> h: (self.hidden_size, 1)\n",
        "    h = torch.zeros((self.hidden_size, 1))\n",
        "\n",
        "    # store information for backward pass:\n",
        "\n",
        "    # store the inputs which represent all sequences\n",
        "    self.all_sequences = inputs\n",
        "\n",
        "    # initialise a dict to store all hidden states, for this dict the key is the index of the time step,\n",
        "    # and the value is the hidden state (h) at that step. Our first hidden state is the initialised one above,\n",
        "    # and because we haven't started processing any input yet, we will give it time step index 0.\n",
        "    self.previous_hidden_states = {0:h}\n",
        "\n",
        "    # Perform RNN for each time step, the input for each time step is x\n",
        "    for i, x in enumerate(inputs):\n",
        "      # calculate the next hidden state using equation 1\n",
        "      # W (hidden_size, input_size) @ x (input_size, 1) -> (hidden_size, 1)\n",
        "      # U (hidden_size, hidden_size) @ h (hidden_size, 1) -> (hidden_size, 1)\n",
        "      h = torch.tanh(self.U @ h + self.W @ x + self.BU)\n",
        "\n",
        "      # store this hidden state in self.previous_hidden_states.\n",
        "      # we already indexed the hidden state at step 0, the for loop counts from (0->(N-1))\n",
        "      # we need to take care of that by adding 1 to the for loop counter.\n",
        "      self.previous_hidden_states[i+1] = h\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Hidden state size: {h.shape}\")\n",
        "      print(f\"Hidden states: {self.previous_hidden_states}\")\n",
        "\n",
        "    # Compute the output using equation 2 and the last hidden state\n",
        "    # V (output_size, hidden_size) @ h (hidden_size, 1) -> (output_size, 1)\n",
        "    # Softmax has an argument called 'dim' that specifies the index dimension for applying softmax.\n",
        "    # Applying softmax to the 'output_size' dimension provides the label distribution (which we are intreseted in).\n",
        "    # Determine the index of the 'output_size' dimension to use as the value for 'dim'.\n",
        "    y = torch.softmax((self.V @ h + self.BV), dim=0)\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Output size: {y.shape}\")\n",
        "\n",
        "    # return the output (y) and the last hidden state (h)\n",
        "    return y, h\n",
        "\n",
        "    ## END OF YOUR CODE ##\n",
        "\n",
        "  # Step 4: Perform Backward propagation\n",
        "  def backward_through_time(self, outputs, y, learn_rate=2e-2):\n",
        "    '''\n",
        "    Perform a backward pass of the RNN.\n",
        "    - outputs: network predictions.\n",
        "    - y: ground truth labels (targets).\n",
        "    - learn_rate is a float.\n",
        "    '''\n",
        "    n = len(self.all_sequences)\n",
        "\n",
        "    # Backpropagate through Output Layer:\n",
        "    #target_labels or y  = 0 #index of correct label\n",
        "    #outputs =[0.2, 0.8] #predictions\n",
        "    #we will copy the outputs to variable d_y to indicate that,\n",
        "    #we want the change in prediction compared to the target index\n",
        "    #so that we can propagate this change to the network, to make better predictions the next time.\n",
        "    #so d_y[y] = -0.8 (0.2 - 1)  and this is what our neural network needs to care about.\n",
        "    d_y = outputs\n",
        "    d_y[y] -= 1\n",
        "\n",
        "    # Calculate dL/dV and dL/dBV.\n",
        "    # Since the output vectors and the one-hot encoded target vectors(y) have different dimensions,\n",
        "    # we perform the matrix multiplication using the transpose of the hidden state matrix (self.previous_hidden_states[n].T),\n",
        "    # which matches the dimensionality of the output vectors.\n",
        "    d_V = d_y @ self.previous_hidden_states[n].T\n",
        "    d_BV = d_y\n",
        "\n",
        "    # Backpropagate through time:\n",
        "    # Initialise dL/dU, dL/dW, and dL/dBU to zero, because those are the sets of parameters,\n",
        "    # to accumulate their gradients across all time steps.\n",
        "    d_U = torch.zeros(self.U.shape)\n",
        "    d_W = torch.zeros(self.W.shape)\n",
        "    d_BU = torch.zeros(self.BU.shape)\n",
        "\n",
        "    # Calculate dL/dh for the last h.\n",
        "    # dL/dh = dL/dy * dy/dh\n",
        "    d_h = self.V.T @ d_y\n",
        "\n",
        "    for t in reversed(range(n)):\n",
        "      # Backpropagate through Hidden Layer:\n",
        "      # An intermediate value: dL/dh * (1 - h^2)\n",
        "      # (1 - h^2) is Tanh derivative\n",
        "      d_h_raw = ((1 - self.previous_hidden_states[t + 1] ** 2) * d_h)\n",
        "\n",
        "      # dL/dBU = dL/dh * (1 - h^2)\n",
        "      d_BU += d_h_raw\n",
        "\n",
        "      # dL/dU = dL/dh * (1 - h^2) * h_{t-1}\n",
        "      d_U += d_h_raw @ self.previous_hidden_states[t].T\n",
        "\n",
        "      # dL/dW = dL/dh * (1 - h^2) * x\n",
        "      d_W += d_h_raw @ self.all_sequences[t].T\n",
        "\n",
        "      # Update hidden state gradient for next time step:\n",
        "      # Next dL/dh = dL/dh * (1 - h^2) * U\n",
        "      d_h = self.U @ d_h_raw\n",
        "\n",
        "    # Clip to prevent exploding gradients. RNN and its variants suffer alot from this problem\n",
        "    for d in [d_W, d_U, d_V, d_BU, d_BV]:\n",
        "      torch.clip(d, -1, 1, out=d)\n",
        "\n",
        "    # Update weights and biases using gradient descent.\n",
        "    self.U -= learn_rate * d_U\n",
        "    self.W -= learn_rate * d_W\n",
        "    self.V -= learn_rate * d_V\n",
        "    self.BU -= learn_rate * d_BU\n",
        "    self.BV -= learn_rate * d_BV\n",
        "\n",
        "  # Step 5: Training Loop\n",
        "  def train(self, trainset_inputs: List[List[torch.tensor]], trainset_targets: List[List[torch.tensor]], epochs: int=100, learning_rate: float=0.01) -> None:\n",
        "          '''\n",
        "              Trains the Elman-style RNN using the specified training data.\n",
        "\n",
        "              Args:\n",
        "                trainset_inputs: contain input samples\n",
        "                trainset_tragets: contain the corresponding labels.\n",
        "                epochs (int): Number of training epochs. Default to 100.\n",
        "                learning_rate (float): Learning rate for weight updates. Default to 0.01.\n",
        "          '''\n",
        "\n",
        "          self.losses = []\n",
        "          for i, epoch in enumerate(range(epochs)):\n",
        "              loss = 0\n",
        "              num_correct = 0\n",
        "              for x, y in zip(trainset_inputs, trainset_targets):\n",
        "                  # Forward pass\n",
        "                  outputs, _ = self.forward(x)\n",
        "                  # Compute cross entropy loss/ Accuracy\n",
        "                  loss -= torch.log(outputs[y])\n",
        "                  num_correct += int(torch.argmax(outputs) == y)\n",
        "\n",
        "                  # Backward pass through time\n",
        "                  self.backward_through_time(outputs, y)\n",
        "\n",
        "              self.losses.append((loss / len(trainset_inputs)).item())\n",
        "              print(f\"Train set accuracy at final Epoch {i+1}: {num_correct / len(trainset_inputs)}\")\n",
        "\n",
        "\n",
        "  def show_learning_curve(self) -> None:\n",
        "        '''\n",
        "        Displays the learning curve (loss curve) during training.\n",
        "        '''\n",
        "        plt.plot(self.losses)\n",
        "        plt.xlabel('Epochs') # x axis label\n",
        "        plt.ylabel('Cross-Entropy Loss') # y axis label\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "  def test(self, testset_inputs: List[List[torch.tensor]], testset_targets: List[List[torch.tensor]]) -> float:\n",
        "        '''\n",
        "        Calculates the predictions for the given samples.\n",
        "\n",
        "        Args:\n",
        "            testset_inputs: contains test samples.\n",
        "            testset_targets: contain the corresponding labels\n",
        "\n",
        "        Returns:\n",
        "            Accuracy: int\n",
        "        '''\n",
        "        num_correct = 0\n",
        "        for x, y in zip(testset_inputs, testset_targets):\n",
        "            y_pred, _ = self.forward(x)\n",
        "            num_correct += int(torch.argmax(y_pred) == y)\n",
        "        return num_correct / len(testset_inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5_nYqXhewiy"
      },
      "source": [
        "As we explained in the Week 2 Lab, we should test our neural network using a dummy dataset to ensure that everything is working as intended before committing to training on a real dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "oL22BHXcjS5d"
      },
      "outputs": [],
      "source": [
        "# Assuming X is a sentence that contains a variable length of words, and each words has specific hidden size.\n",
        "# RNN will receive each word sequentially at a different time step and maintain a hidden state for them.\n",
        "\n",
        "sequence_length = 3 # we have three words in this sentence\n",
        "X = [torch.randn(18, 1)] * sequence_length  # Dummy input data\n",
        "\n",
        "#Assuming y classify our sentence to two classes (positive or negative)\n",
        "y = torch.randint(0, 2, size=(1,))  #Dummy target labels (2 classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "gwpPa2cBlIJI"
      },
      "outputs": [],
      "source": [
        "input_size = 18\n",
        "output_size = 2 #num classes\n",
        "# hidden size is by default set to 64, feel free to tweak it\n",
        "model = ElmanRNN(input_size, output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "hGPQIb0olL04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden state size: torch.Size([64, 1])\n",
            "Hidden states: {0: tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]]), 1: tensor([[-0.0006],\n",
            "        [ 0.0023],\n",
            "        [ 0.0072],\n",
            "        [ 0.0049],\n",
            "        [-0.0012],\n",
            "        [-0.0008],\n",
            "        [-0.0055],\n",
            "        [ 0.0013],\n",
            "        [ 0.0070],\n",
            "        [ 0.0038],\n",
            "        [ 0.0095],\n",
            "        [-0.0031],\n",
            "        [-0.0006],\n",
            "        [ 0.0029],\n",
            "        [-0.0062],\n",
            "        [-0.0022],\n",
            "        [ 0.0022],\n",
            "        [ 0.0015],\n",
            "        [-0.0071],\n",
            "        [-0.0083],\n",
            "        [-0.0021],\n",
            "        [-0.0005],\n",
            "        [-0.0001],\n",
            "        [-0.0049],\n",
            "        [ 0.0019],\n",
            "        [ 0.0017],\n",
            "        [ 0.0003],\n",
            "        [-0.0005],\n",
            "        [ 0.0084],\n",
            "        [ 0.0047],\n",
            "        [ 0.0043],\n",
            "        [ 0.0036],\n",
            "        [-0.0028],\n",
            "        [-0.0037],\n",
            "        [-0.0024],\n",
            "        [-0.0026],\n",
            "        [ 0.0005],\n",
            "        [ 0.0070],\n",
            "        [-0.0010],\n",
            "        [ 0.0026],\n",
            "        [-0.0050],\n",
            "        [ 0.0001],\n",
            "        [-0.0005],\n",
            "        [-0.0010],\n",
            "        [-0.0021],\n",
            "        [-0.0011],\n",
            "        [-0.0057],\n",
            "        [ 0.0018],\n",
            "        [ 0.0078],\n",
            "        [ 0.0062],\n",
            "        [ 0.0007],\n",
            "        [-0.0041],\n",
            "        [ 0.0021],\n",
            "        [-0.0007],\n",
            "        [ 0.0016],\n",
            "        [ 0.0053],\n",
            "        [ 0.0015],\n",
            "        [-0.0014],\n",
            "        [ 0.0066],\n",
            "        [ 0.0042],\n",
            "        [ 0.0009],\n",
            "        [ 0.0070],\n",
            "        [-0.0036],\n",
            "        [ 0.0024]]), 2: tensor([[-0.0006],\n",
            "        [ 0.0022],\n",
            "        [ 0.0072],\n",
            "        [ 0.0049],\n",
            "        [-0.0012],\n",
            "        [-0.0008],\n",
            "        [-0.0055],\n",
            "        [ 0.0013],\n",
            "        [ 0.0070],\n",
            "        [ 0.0038],\n",
            "        [ 0.0095],\n",
            "        [-0.0031],\n",
            "        [-0.0006],\n",
            "        [ 0.0029],\n",
            "        [-0.0062],\n",
            "        [-0.0021],\n",
            "        [ 0.0022],\n",
            "        [ 0.0015],\n",
            "        [-0.0071],\n",
            "        [-0.0083],\n",
            "        [-0.0021],\n",
            "        [-0.0006],\n",
            "        [-0.0002],\n",
            "        [-0.0049],\n",
            "        [ 0.0018],\n",
            "        [ 0.0017],\n",
            "        [ 0.0003],\n",
            "        [-0.0005],\n",
            "        [ 0.0083],\n",
            "        [ 0.0047],\n",
            "        [ 0.0043],\n",
            "        [ 0.0037],\n",
            "        [-0.0028],\n",
            "        [-0.0036],\n",
            "        [-0.0024],\n",
            "        [-0.0026],\n",
            "        [ 0.0005],\n",
            "        [ 0.0070],\n",
            "        [-0.0009],\n",
            "        [ 0.0026],\n",
            "        [-0.0050],\n",
            "        [ 0.0002],\n",
            "        [-0.0005],\n",
            "        [-0.0010],\n",
            "        [-0.0021],\n",
            "        [-0.0011],\n",
            "        [-0.0057],\n",
            "        [ 0.0018],\n",
            "        [ 0.0078],\n",
            "        [ 0.0062],\n",
            "        [ 0.0007],\n",
            "        [-0.0041],\n",
            "        [ 0.0021],\n",
            "        [-0.0007],\n",
            "        [ 0.0016],\n",
            "        [ 0.0053],\n",
            "        [ 0.0015],\n",
            "        [-0.0014],\n",
            "        [ 0.0066],\n",
            "        [ 0.0042],\n",
            "        [ 0.0009],\n",
            "        [ 0.0070],\n",
            "        [-0.0036],\n",
            "        [ 0.0024]]), 3: tensor([[-0.0006],\n",
            "        [ 0.0022],\n",
            "        [ 0.0072],\n",
            "        [ 0.0049],\n",
            "        [-0.0012],\n",
            "        [-0.0008],\n",
            "        [-0.0055],\n",
            "        [ 0.0013],\n",
            "        [ 0.0070],\n",
            "        [ 0.0038],\n",
            "        [ 0.0095],\n",
            "        [-0.0031],\n",
            "        [-0.0006],\n",
            "        [ 0.0029],\n",
            "        [-0.0062],\n",
            "        [-0.0021],\n",
            "        [ 0.0022],\n",
            "        [ 0.0015],\n",
            "        [-0.0071],\n",
            "        [-0.0083],\n",
            "        [-0.0021],\n",
            "        [-0.0006],\n",
            "        [-0.0002],\n",
            "        [-0.0049],\n",
            "        [ 0.0018],\n",
            "        [ 0.0017],\n",
            "        [ 0.0003],\n",
            "        [-0.0005],\n",
            "        [ 0.0083],\n",
            "        [ 0.0047],\n",
            "        [ 0.0043],\n",
            "        [ 0.0037],\n",
            "        [-0.0028],\n",
            "        [-0.0036],\n",
            "        [-0.0024],\n",
            "        [-0.0026],\n",
            "        [ 0.0005],\n",
            "        [ 0.0070],\n",
            "        [-0.0009],\n",
            "        [ 0.0026],\n",
            "        [-0.0050],\n",
            "        [ 0.0002],\n",
            "        [-0.0005],\n",
            "        [-0.0010],\n",
            "        [-0.0021],\n",
            "        [-0.0011],\n",
            "        [-0.0057],\n",
            "        [ 0.0018],\n",
            "        [ 0.0078],\n",
            "        [ 0.0062],\n",
            "        [ 0.0007],\n",
            "        [-0.0041],\n",
            "        [ 0.0021],\n",
            "        [-0.0007],\n",
            "        [ 0.0016],\n",
            "        [ 0.0053],\n",
            "        [ 0.0015],\n",
            "        [-0.0014],\n",
            "        [ 0.0066],\n",
            "        [ 0.0042],\n",
            "        [ 0.0009],\n",
            "        [ 0.0070],\n",
            "        [-0.0036],\n",
            "        [ 0.0024]])}\n",
            "Output size: torch.Size([2, 1])\n",
            "tensor([[0.5000],\n",
            "        [0.5000]])\n"
          ]
        }
      ],
      "source": [
        "# we enable verbose, to print the matrix sizes in the forward function\n",
        "predictions, hidden_state = model.forward(X, verbose=True)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFVFEx3loq5A"
      },
      "source": [
        "Now let's try the backward pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "hx5_57rFkRo-"
      },
      "outputs": [],
      "source": [
        "# if this runs without error, we are safe. backward doesn't return any value,\n",
        "# just changing the global variables of the parameters.\n",
        "model.backward_through_time(predictions, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF7iWWMjlnPX"
      },
      "source": [
        "Seems our RNN is working as intended, now let's go ahead and train a classification model using a real dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nux409nFfnuK"
      },
      "source": [
        "## 2 Train a sentiment classification model with RNNs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drUGpB4-jYLh"
      },
      "source": [
        "#### 2.1 Dataset\n",
        "\n",
        "The overall goal of sentiment classification is to automatically identify the sentiment conveyed by a text, and assign each text to predefined categories such as positive and negative (below True and False, respectively).\n",
        "\n",
        "We use the following [data](https://github.com/ramnathv/rnn-from-scratch-1/blob/master/data.py) to train our RNN. In the next cell you will see two dictionaries, the first for the train set with 58 train items, and the second for the test set with 20 test items, each containing the input text as the dictionary key and the class label (True=positive or False=negative) as the corrsponding value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "-tLfcMEXM0j-"
      },
      "outputs": [],
      "source": [
        "train_data = {\n",
        "  'good': True,\n",
        "  'bad': False,\n",
        "  'happy': True,\n",
        "  'sad': False,\n",
        "  'not good': False,\n",
        "  'not bad': True,\n",
        "  'not happy': False,\n",
        "  'not sad': True,\n",
        "  'very good': True,\n",
        "  'very bad': False,\n",
        "  'very happy': True,\n",
        "  'very sad': False,\n",
        "  'i am happy': True,\n",
        "  'this is good': True,\n",
        "  'i am bad': False,\n",
        "  'this is bad': False,\n",
        "  'i am sad': False,\n",
        "  'this is sad': False,\n",
        "  'i am not happy': False,\n",
        "  'this is not good': False,\n",
        "  'i am not bad': True,\n",
        "  'this is not sad': True,\n",
        "  'i am very happy': True,\n",
        "  'this is very good': True,\n",
        "  'i am very bad': False,\n",
        "  'this is very sad': False,\n",
        "  'this is very happy': True,\n",
        "  'i am good not bad': True,\n",
        "  'this is good not bad': True,\n",
        "  'i am bad not good': False,\n",
        "  'i am good and happy': True,\n",
        "  'this is not good and not happy': False,\n",
        "  'i am not at all good': False,\n",
        "  'i am not at all bad': True,\n",
        "  'i am not at all happy': False,\n",
        "  'this is not at all sad': True,\n",
        "  'this is not at all happy': False,\n",
        "  'i am good right now': True,\n",
        "  'i am bad right now': False,\n",
        "  'this is bad right now': False,\n",
        "  'i am sad right now': False,\n",
        "  'i was good earlier': True,\n",
        "  'i was happy earlier': True,\n",
        "  'i was bad earlier': False,\n",
        "  'i was sad earlier': False,\n",
        "  'i am very bad right now': False,\n",
        "  'this is very good right now': True,\n",
        "  'this is very sad right now': False,\n",
        "  'this was bad earlier': False,\n",
        "  'this was very good earlier': True,\n",
        "  'this was very bad earlier': False,\n",
        "  'this was very happy earlier': True,\n",
        "  'this was very sad earlier': False,\n",
        "  'i was good and not bad earlier': True,\n",
        "  'i was not good and not happy earlier': False,\n",
        "  'i am not at all bad or sad right now': True,\n",
        "  'i am not at all good or happy right now': False,\n",
        "  'this was not happy and not good earlier': False,\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "  'this is happy': True,\n",
        "  'i am good': True,\n",
        "  'this is not happy': False,\n",
        "  'i am not good': False,\n",
        "  'this is not bad': True,\n",
        "  'i am not sad': True,\n",
        "  'i am very good': True,\n",
        "  'this is very bad': False,\n",
        "  'i am very sad': False,\n",
        "  'this is bad not good': False,\n",
        "  'this is good and happy': True,\n",
        "  'i am not good and not happy': False,\n",
        "  'i am not at all sad': True,\n",
        "  'this is not at all good': False,\n",
        "  'this is not at all bad': True,\n",
        "  'this is good right now': True,\n",
        "  'this is sad right now': False,\n",
        "  'this is very bad right now': False,\n",
        "  'this was good earlier': True,\n",
        "  'i was not happy and not good earlier': False,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTupTcUVf_rc"
      },
      "source": [
        "To encode our text data into a numerical format, we will follow a two-step process:\n",
        "\n",
        "- Indexing (i.e. tokenizing): We will assign a unique index to each word in the text corpus. This step involves creating a dictionary where each word is mapped to a unique integer index.\n",
        "\n",
        "- One-Hot Encoding (vector representation of words): After indexing the words, we will represent each word as a one-hot encoded vector. In this encoding scheme, each word is represented as a binary vector where all elements are zero except for the index corresponding to the word, which is set to one.\n",
        "\n",
        "For example:\n",
        "Word to Index Mapping: {'Recurrent': 0, 'Neural': 1, 'Network': 2}\n",
        "One-Hot Encoded Data:\n",
        "- Recurrent: [1, 0, 0]\n",
        "- Neural:    [0, 1, 0]\n",
        "- Network:   [0, 0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Ib8P3j1Pf37c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18 unique words found\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Create the vocabulary.\n",
        "\n",
        "# get unique words in the training corpus: set() removes duplicates\n",
        "vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))\n",
        "vocab_size = len(vocab)\n",
        "print('%d unique words found' % vocab_size)\n",
        "\n",
        "# Assign indices to each word.\n",
        "word_to_idx = { w: i for i, w in enumerate(vocab) }\n",
        "# test getting the index for one of the words from our dataset\n",
        "print(word_to_idx['good'])\n",
        "\n",
        "def createInput(text):\n",
        "  '''\n",
        "  Returns an array of one-hot vectors representing the words in the input text string.\n",
        "  - text is a string\n",
        "  - Each one-hot vector has shape (vocab_size, 1)\n",
        "  '''\n",
        "  inputs = []\n",
        "  for w in text.split(' '):\n",
        "    v = torch.zeros(vocab_size, 1)\n",
        "    v[word_to_idx[w]] = 1\n",
        "    inputs.append(v)\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "5EtaFe666rmD"
      },
      "outputs": [],
      "source": [
        "def processData(data: dict)-> tuple:\n",
        "  '''\n",
        "    Process the input data for training or inference.\n",
        "\n",
        "    Args:\n",
        "        data (dict): A dictionary containing input-output pairs.\n",
        "    Returns:\n",
        "        tuple: A tuple containing processed inputs and targets.\n",
        "  '''\n",
        "  items = list(data.items())\n",
        "  random.shuffle(items)\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  for x, y in items:\n",
        "    inputs.append(createInput(x))\n",
        "    targets.append(int(y))\n",
        "  return inputs, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEz0qnsCspL2"
      },
      "source": [
        "### 2.2 Initialise a new instance of the ElmanRNN  and set the the proper sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "NSYUSTHmhA0u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set accuracy at final Epoch 1: 0.5344827586206896\n",
            "Train set accuracy at final Epoch 2: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 3: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 4: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 5: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 6: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 7: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 8: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 9: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 10: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 11: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 12: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 13: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 14: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 15: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 16: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 17: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 18: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 19: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 20: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 21: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 22: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 23: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 24: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 25: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 26: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 27: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 28: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 29: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 30: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 31: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 32: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 33: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 34: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 35: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 36: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 37: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 38: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 39: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 40: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 41: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 42: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 43: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 44: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 45: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 46: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 47: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 48: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 49: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 50: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 51: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 52: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 53: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 54: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 55: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 56: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 57: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 58: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 59: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 60: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 61: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 62: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 63: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 64: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 65: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 66: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 67: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 68: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 69: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 70: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 71: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 72: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 73: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 74: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 75: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 76: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 77: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 78: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 79: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 80: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 81: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 82: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 83: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 84: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 85: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 86: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 87: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 88: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 89: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 90: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 91: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 92: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 93: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 94: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 95: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 96: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 97: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 98: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 99: 0.5517241379310345\n",
            "Train set accuracy at final Epoch 100: 0.5517241379310345\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x13ab4fc50>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## INSERT YOUR CODE HERE ##\n",
        "# Initialise our Elman-style RNN\n",
        "input_size = vocab_size # number of words in the vocabulary\n",
        "output_size =  2 #num classes\n",
        "# hidden size is by default set to 64, feel free to tweak it\n",
        "model = ElmanRNN(input_size, output_size)\n",
        "\n",
        "\n",
        "# before start training, process train set dict using processData function:\n",
        "x_train, y_train = processData(train_data)\n",
        "\n",
        "#train the Elman-style RNN\n",
        "model.train(x_train, y_train, epochs=100, learning_rate=0.01)\n",
        "'''predictions, hidden_state = model.forward(X, verbose=True)\n",
        "print(predictions)\n",
        "model.backward_through_time(predictions, y_train)'''\n",
        "\n",
        "#plot the learning curve\n",
        "plt.plot(model.losses, label='Train')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Learning Curve - Loss')\n",
        "plt.legend()\n",
        "\n",
        "## END OF YOUR CODE ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig3hTbCvxWQa"
      },
      "source": [
        "### 2.3 Evaluate the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "4KgyB3L_lcg0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "## INSERT YOUR CODE HERE ##\n",
        "\n",
        "# process test set dict using processData function:\n",
        "x_test, y_test = processData(test_data)\n",
        "\n",
        "# test your model\n",
        "\n",
        "accuracy = model.test(x_test, y_test)\n",
        "\n",
        "# print Accuracy\n",
        "print(f\"Test set accuracy: {accuracy}\")\n",
        "## END OF YOUR CODE ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcwUSghLNGvw"
      },
      "source": [
        "## Week 4 Submission Task: Improve performance and explore different text encoding techniques\n",
        "\n",
        "In previous labs, we explored various techniques for encoding text into numerical values.\n",
        "In the Week 1 Lab, the natural logarithm of text length and the count of sport-related words were utilised. In the Week 2 Lab, TF-IDF was applied. In this Lab, one-hot encoding was used above.\n",
        "\n",
        "To improve the performance of the network, your submission task is to add an embeddings layer to map the one-hot representations to word embeddings via an embeddings matrix E. The embeddings will then be multiplied with the weight matrix W, to feed into the hidden layer at the current time step.\n",
        "\n",
        "Extension task (voluntary): There are are other ways you can try to improve performance, including different initialisations, increasing the training set size, reducing overfitting, and so on. Feel free to try any and all of these.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU7k6L6PL3aQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnF13fbPIark"
      },
      "source": [
        "# Resources\n",
        "\n",
        "- Speech and Language Processing (3rd ed. draft) Dan Jurafsky and James H. Martin [Chapter 13](https://web.stanford.edu/~jurafsky/slp3/13.pdf)\n",
        "- Goodfellow et al., [Sections 10.210.2.2](https://www.deeplearningbook.org/contents/rnn.html)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
